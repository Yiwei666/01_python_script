# 无监督学习

# 1. 主成分分析（PCA）

### 1. 协方差矩阵

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-155357.png" alt="Image Description" width="700">
</p>


### 2. 身高-体重举例

1. 绘制特征向量方向

在绘制特征向量时，我们需要将特征向量的方向和长度表示出来，使其在图形上可视化。特征向量方向表明了数据主要变异的方向，而特征向量的长度则与对应的特征值的平方根成正比，这样可以更直观地反映出每个方向上数据的分散程度。

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240617-104522.png" alt="Image Description" width="500">
</p>


- 特征向量的长度与特征值的关系

在上面的代码中，特征向量被缩放了 `2 * np.sqrt(eigenvalues[i])` 倍。这里的 2 是为了在图中更清晰地显示特征向量，而 `np.sqrt(eigenvalues[i])` 则是因为特征值的大小表示了在对应的特征向量方向上的数据方差。特征向量原本的长度是1（因为它们是单位向量），但我们通过乘以特征值的平方根来调整其长度，从而使得向量的长度在图形中反映出在该方向上的数据变异性大小。

- 特征向量的方向

特征向量的方向表示数据分布的主要方向。在二维空间中，每个特征向量都可以从数据中心指向一个特定方向。代码中使用 `plt.quiver` 函数绘制箭头来表示这些向量。`plt.quiver` 需要起点的 x, y 坐标（这里是数据的中心，`height_mean` 和 `weight_mean`），以及向量的 x, y 分量（`vec[0] 和 vec[1]`）。`angles='xy', scale_units='xy', scale=1` 这些参数确保向量按照 x, y 轴的比例正确绘制，而不是根据图的比例自动缩放。


```py
import numpy as np
import matplotlib.pyplot as plt

# 设置随机种子，以便每次运行代码时获得相同的结果
np.random.seed(0)

# 生成模拟数据
num_points = 100
height = np.random.normal(170, 10, num_points)  # 身高，均值170，标准差10
weight = height * 0.5 + np.random.normal(60, 5, num_points)  # 体重，与身高正相关

# 将数据中心化
height_mean = np.mean(height)
weight_mean = np.mean(weight)
height_centered = height - height_mean
weight_centered = weight - weight_mean

# 组装成一个二维数据集
data = np.vstack([height_centered, weight_centered]).T

# 计算协方巨阵
cov_matrix = np.cov(data, rowvar=False)
print("Covariance Matrix:\n", cov_matrix)

# 计算特征值和特征向量
eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
print("Eigenvalues before sorting:\n", eigenvalues)
print("Eigenvectors before sorting:\n", eigenvectors)

# 对特征值和特征向量排序（由小到大）
sorted_indices = np.argsort(eigenvalues)
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]

print("Sorted Eigenvalues:\n", eigenvalues)
print("Sorted Eigenvectors:\n", eigenvectors)

# 绘制原始数据
plt.scatter(height, weight, alpha=0.7)
plt.xlabel('Height (cm)')
plt.ylabel('Weight (kg)')
plt.title('Height vs Weight with Principal Components')

# 添加特征向量方向
for i in range(len(eigenvalues)):
    vec = eigenvectors[:, i] * 2 * np.sqrt(eigenvalues[i])  # 缩放特征向量以便可视化
    plt.quiver(height_mean, weight_mean, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, color=['r', 'b'])

plt.grid(True)
plt.axis('equal')
plt.show()
```

2. 绘制投影后的数据

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240617-105431.png" alt="Image Description" width="700">
</p>



解释

- 原始数据：散点图显示了身高和体重的原始分布。
- 降维后的数据：所有数据点都被投影到一个一维线上（实际上是选择的主成分方向）。这样，我们就捕获了数据中最主要的变异方向，而忽略了其他方向的小变异。

```py
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)
num_points = 100
height = np.random.normal(170, 10, num_points)
weight = height * 0.5 + np.random.normal(60, 5, num_points)

height_centered = height - np.mean(height)
weight_centered = weight - np.mean(weight)
data = np.vstack([height_centered, weight_centered]).T

cov_matrix = np.cov(data, rowvar=False)
eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

# 选择最大特征值对应的特征向量
max_eigenvector = eigenvectors[:, np.argmax(eigenvalues)]

# 投影数据到这个特征向量
projected_data = data @ max_eigenvector

# 绘制原始数据
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(height, weight, alpha=0.7)
plt.xlabel('Height (cm)')
plt.ylabel('Weight (kg)')
plt.title('Original Data')

# 绘制降维后的数据
plt.subplot(1, 2, 2)
plt.scatter(projected_data, np.zeros_like(projected_data), alpha=0.7)
plt.xlabel('Projected Data')
plt.title('Data After Dimension Reduction')
plt.yticks([])
plt.tight_layout()
plt.show()
```


### 3. 鸢尾花PCA

接下来，我将使用 `scikit-learn` 的鸢尾花数据集（Iris）作为示例，来展示如何使用PCA进行数据分析和可视化。这个数据集包含150个样本，每个样本有4个特征，分别是花萼长度、花萼宽度、花瓣长度和花瓣宽度。

1. 手动实现PCA代码

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.preprocessing import StandardScaler

# 加载数据
iris = datasets.load_iris()
X = iris.data

# 步骤 1: 数据标准化
scaler = StandardScaler()
X_std = scaler.fit_transform(X)
print("标准化后的数据：")
print(X_std[:5])  # 打印前5行数据

# 步骤 2: 计算协方差矩阵
cov_matrix = np.cov(X_std.T)
print("\n协方差矩阵：")
print(cov_matrix)

# 步骤 3: 计算协方差矩阵的特征值和特征向量
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
print("\n特征值：")
print(eigenvalues)
print("\n特征向量：")
print(eigenvectors)

# 步骤 4: 对特征向量排序，并选择主要特征向量
# 排序特征值并提取前两个最大的
indices = np.argsort(eigenvalues)[::-1]
eigenvalues_sorted = eigenvalues[indices]
eigenvectors_sorted = eigenvectors[:, indices]

print("\n排序后的特征值：")
print(eigenvalues_sorted)
print("\n排序后的特征向量：")
print(eigenvectors_sorted)

# 选择前两个特征向量
projection_matrix = eigenvectors_sorted[:, :2]
print("\n投影矩阵 (前两个特征向量)：")
print(projection_matrix)

# 步骤 5: 数据转换
X_pca = X_std.dot(projection_matrix)
print("\n降维后的数据：")
print(X_pca[:5])  # 打印前5行数据

# 可视化结果
plt.figure(figsize=(8, 6))
for i, target_name in zip([0, 1, 2], iris.target_names):
    plt.scatter(X_pca[iris.target == i, 0], X_pca[iris.target == i, 1], label=target_name)
plt.legend()
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of IRIS Dataset (Manual Steps)')
plt.show()
```

2. 使用sklearn的PCA类

```py
# 导入必要的库
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# 加载鸢尾花数据集
data = load_iris()
X = data.data
y = data.target
target_names = data.target_names

# 创建PCA对象，设定降维后的主成分数量为2
pca = PCA(n_components=2)

# 对数据进行降维
X_r = pca.fit_transform(X)

# 可视化
plt.figure()
colors = ['navy', 'turquoise', 'darkorange']
lw = 2

for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,
                label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('PCA of IRIS dataset')
plt.show()
```



### 4. 数据分析

1. 投影数据可视化

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-161210.png" alt="Image Description" width="450">
</p>

2. 数据分析

解释：

- **标准化后的数据**：对原始数据进行标准化，使每个特征的平均值为0，标准差为1。打印标准化后的数据的前5行。
- **协方差矩阵**：计算标准化数据的协方差矩阵，显示各个特征之间的协方差。
- **特征值和特征向量**：计算协方差矩阵的特征值和特征向量，并打印它们。
- **排序后的特征值和特征向量**：按照特征值的大小对特征值和特征向量进行排序，并打印排序结果。
- **投影矩阵**：选择前两个特征向量，构成投影矩阵，并打印出来。
- **降维后的数据**：使用投影矩阵将标准化后的数据转换到新的二维空间，并打印转换后的数据的前5行。


3. 过程数据

```
标准化后的数据：
[[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]
 [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]
 [-1.38535265  0.32841405 -1.39706395 -1.3154443 ]
 [-1.50652052  0.09821729 -1.2833891  -1.3154443 ]
 [-1.02184904  1.24920112 -1.34022653 -1.3154443 ]]

协方差矩阵：
[[ 1.00671141 -0.11835884  0.87760447  0.82343066]
 [-0.11835884  1.00671141 -0.43131554 -0.36858315]
 [ 0.87760447 -0.43131554  1.00671141  0.96932762]
 [ 0.82343066 -0.36858315  0.96932762  1.00671141]]

特征值：
[2.93808505 0.9201649  0.14774182 0.02085386]

特征向量：
[[ 0.52106591 -0.37741762 -0.71956635  0.26128628]
 [-0.26934744 -0.92329566  0.24438178 -0.12350962]
 [ 0.5804131  -0.02449161  0.14212637 -0.80144925]
 [ 0.56485654 -0.06694199  0.63427274  0.52359713]]

排序后的特征值：
[2.93808505 0.9201649  0.14774182 0.02085386]

排序后的特征向量：
[[ 0.52106591 -0.37741762 -0.71956635  0.26128628]
 [-0.26934744 -0.92329566  0.24438178 -0.12350962]
 [ 0.5804131  -0.02449161  0.14212637 -0.80144925]
 [ 0.56485654 -0.06694199  0.63427274  0.52359713]]

投影矩阵 (前两个特征向量)：
[[ 0.52106591 -0.37741762]
 [-0.26934744 -0.92329566]
 [ 0.5804131  -0.02449161]
 [ 0.56485654 -0.06694199]]

降维后的数据：
[[-2.26470281 -0.4800266 ]
 [-2.08096115  0.67413356]
 [-2.36422905  0.34190802]
 [-2.29938422  0.59739451]
 [-2.38984217 -0.64683538]]
```



# 2. 潜在语义分析 (LSA)

### 1. 基本概念

LSA 是一种基于矩阵分解的技术，通过对`词-文档`矩阵进行奇异值分解（SVD），将高维的文本数据映射到一个低维的潜在语义空间中。这种方法能够捕捉`词语和文档`之间的隐含关系，从而解决词义多样性和同义词问题。


### 2. 具体步骤

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-171135.png" alt="Image Description" width="700">
</p>

### 3. TF-IDF 特征矩阵

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-193619.png" alt="Image Description" width="700">
</p>



# 3. 非负矩阵分解

### 1. 基本概念

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-213002.png" alt="Image Description" width="700">
</p>

### 2. 具体步骤

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-194313.png" alt="Image Description" width="700">
</p>

### 3. 代码示例


```py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# 定义小型数据集
documents = [
    "The cat sat on the mat",
    "The dog sat on the log",
    "Cats and dogs are pets",
    "Dogs and cats are animals",
    "The mat and the log are objects"
]

# 使用TF-IDF向量化文本数据
vectorizer = TfidfVectorizer(stop_words='english')
X_tfidf = vectorizer.fit_transform(documents)

# 打印TF-IDF矩阵
print("TF-IDF 矩阵：")
print(pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names()))

# 使用NMF进行非负矩阵分解
n_components = 2
nmf_model = NMF(n_components=n_components, random_state=42)
W = nmf_model.fit_transform(X_tfidf)
H = nmf_model.components_

# 创建 DataFrame 以便查看
df_W = pd.DataFrame(W, columns=[f'Component {i+1}' for i in range(n_components)])
df_W['Document'] = documents

df_H = pd.DataFrame(H, columns=vectorizer.get_feature_names())
df_H.index = [f'Component {i+1}' for i in range(n_components)]

# 打印结果矩阵
print("\nW 矩阵：")
print(df_W)
print("\nH 矩阵：")
print(df_H)
```

- 代码分析

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-194720.png" alt="Image Description" width="700">
</p>

### 4. 输出结果

```
TF-IDF 矩阵：
    animals       cat      cats  ...   objects      pets       sat
0  0.000000  0.659118  0.000000  ...  0.000000  0.000000  0.531772
1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.531772
2  0.000000  0.000000  0.531772  ...  0.000000  0.659118  0.000000
3  0.659118  0.000000  0.531772  ...  0.000000  0.000000  0.000000
4  0.000000  0.000000  0.000000  ...  0.659118  0.000000  0.000000

[5 rows x 10 columns]

W 矩阵：
   Component 1  Component 2                         Document
0     0.645813     0.000000           The cat sat on the mat
1     0.645813     0.000000           The dog sat on the log
2     0.000000     0.790957           Cats and dogs are pets
3     0.000000     0.790957        Dogs and cats are animals
4     0.645813     0.000000  The mat and the log are objects

H 矩阵：
              animals     cat      cats  ...  objects      pets       sat
Component 1  0.000000  0.3402  0.000000  ...   0.3402  0.000000  0.548943
Component 2  0.416659  0.0000  0.672315  ...   0.0000  0.416659  0.000000

[2 rows x 10 columns]
```


# 4. 线性判别分析（LDA）

### 1. 基本概念

`线性判别分析（LDA）`是一种统计方法，用于数据的降维和分类，尤其在模式识别和机器学习领域中广泛使用。LDA旨在找到一个线性组合的特征，这些特征能够最好地区分两个或多个类别的对象或事件。它不仅用于降维，还用于分类任务。

- 基本原理

LDA的基本思想是为数据寻找一个最佳的投影方向，这个方向可以最大化类间距离（不同类别的数据分得越远越好）同时最小化类内差异（同一类别的数据尽可能聚集）。在技术上，这是通过优化类间散布矩阵与类内散布矩阵的比值来实现的。

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240617-111129.png" alt="Image Description" width="700">
</p>


### 2. 适用场景

`线性判别分析（Linear Discriminant Analysis，LDA）`是一种经典的监督学习技术，适用于降维和特征提取，尤其适合于分类任务中。以下是LDA适用于降维的数据类型和场景：

1. 有类别标签的数据：
    - LDA是一种监督学习方法，需要每个数据点都有一个对应的类别标签。这使得LDA能够在尝试最大化类间分离的同时，最小化类内的变异。

2. 类别数小于特征数的数据：
    - LDA的效果在类别数较少时更为明显。理论上，LDA最多可以将数据降维到 C−1 维，其中 C 是类别数。因此，当数据集的特征数远多于类别数时，使用LDA降维尤其有用。

3. 希望保持数据类别信息的场景：
    - 由于LDA的目标是最大化类别间的差异，它在保持数据的类别可分性方面表现优秀，适用于那些需要在降维后进行有效分类的应用。

4. 数据特征呈近似高斯分布，各类协方差矩阵相同：
    - LDA假设数据在每个类别中呈高斯分布，且各类别具有相同的协方差矩阵。当这些假设近似成立时，LDA能够实现最佳性能。

5. 二分类或多分类问题：
    - LDA可以用于二分类或多分类问题。在多分类问题中，LDA通过构造多个判别方向来区分不同的类别。




# 5. 混合高斯分布（GMM）

### 1. 基本概念

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240620-150129.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240620-150325.png" alt="Image Description" width="700">
</p>



### 2. 代码示例

- 鸢尾花数据集的GMM聚类结果和聚类效果的轮廓系数评分

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 初始化GMM模型
gmm = GaussianMixture(n_components=3, random_state=0)

# 拟合模型
gmm.fit(X_scaled)

# 预测聚类标签
cluster_labels = gmm.predict(X_scaled)

# 计算轮廓系数
silhouette_avg = silhouette_score(X_scaled, cluster_labels)
print('轮廓系数:', silhouette_avg)

# 可视化聚类结果（只使用前两个特征）
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster_labels, cmap='viridis', marker='o', edgecolors='black')
plt.title('GMM Clustering of Iris Dataset')
plt.xlabel('Feature 1 (standardized)')
plt.ylabel('Feature 2 (standardized)')
plt.colorbar()
plt.show()
```

- 降维后数据可视化

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240621-203649.png" alt="Image Description" width="500">
</p>



# 6. 局部线性嵌入（LLE）

### 1. 基本概念

- `局部线性嵌入（LLE）`是一种流行的非线性降维技术，其核心思想是保持数据的局部邻近性质，而不是像PCA那样关注数据的全局线性结构。LLE尤其适合用于那些在局部区域内近似线性的高维数据，可以帮助揭示复杂的非线性结构。

- LLE算法的核心思想是在低维空间中保持高维空间中每个数据点的局部线性关系。具体来说，算法认为`每个点可以通过其最近邻点的线性组合来近似地表示`。这种方法依赖于一个关键的假设：数据位于一个局部线性的流形上，即在较小的尺度上数据的分布可以通过线性模型来近似。


### 2. 具体步骤

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240621-154009.png" alt="Image Description" width="700">
</p>


### 3. 瑞士卷数据集

`瑞士卷数据集`是一个经常用于演示降维技术的合成数据集，特别是用来展示非线性降维方法的能力。这个数据集的形状类似于卷起来的面团或地毯，是一个三维空间中的二维流形。数据集的结构具有明显的非线性特征，因此常用来测试各种降维算法如何处理非线性关系。

```py
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import LocallyLinearEmbedding

# 生成瑞士卷数据
X, color = make_swiss_roll(n_samples=1500)

# 可视化原始的瑞士卷
fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(121, projection='3d')
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
ax.set_title("Original Swiss Roll")
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

# 应用LLE降维
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=12, method='standard')
X_reduced = lle.fit_transform(X)

# 可视化LLE降维后的结果
ax2 = fig.add_subplot(122)
scatter = ax2.scatter(X_reduced[:, 0], X_reduced[:, 1], c=color, cmap=plt.cm.Spectral)
ax2.set_title("LLE Reduced Swiss Roll")
ax2.set_xlabel('LLE1')
ax2.set_ylabel('LLE2')
plt.colorbar(scatter)

plt.show()
```

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240621-155023.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240621-155426.png" alt="Image Description" width="700">
</p>


# 7. t分布随机邻域嵌入（t-SNE）

### 1. 基本概念



### 2. MNIST手写数字数据集

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240622-142824.png" alt="Image Description" width="700">
</p>

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 步骤1: 加载MNIST数据集
mnist = fetch_openml('mnist_784', version=1)
X = mnist.data.astype(np.float32)  # 转换数据类型
y = mnist.target.astype(np.int32)  # 转换标签类型

# 由于数据集很大，我们仅选用3000个样本来加速计算
np.random.seed(42)
indices = np.random.choice(len(X), 3000, replace=False)
X = X.iloc[indices]
y = y.iloc[indices]

# 步骤2: 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 步骤3: 使用PCA进行预降维以减少计算量
pca = PCA(n_components=50)
X_pca = pca.fit_transform(X_scaled)

# 步骤4: 应用t-SNE进行降维
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_pca)

# 步骤5: 可视化t-SNE的结果
plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='jet', alpha=0.6)
plt.colorbar(scatter)
plt.title('t-SNE visualization of MNIST (3000 samples)')
plt.xlabel('t-SNE-1')
plt.ylabel('t-SNE-2')
plt.grid(True)
plt.show()
```

运行说明：
1. 数据加载：这段代码首先从OpenML加载MNIST数据集。`fetch_openml`函数返回的数据类型是`DataFrame`，我们将其转换为NumPy数组以方便处理。
2. 数据标准化：因为`PCA`和`t-SNE`对数据的缩放很敏感，所以我们先用`StandardScaler`对特征进行标准化处理。
3. PCA预降维：为了减少`t-SNE`的计算量，首先使用PCA将数据从784维降到50维。
4. t-SNE降维：接着使用t-SNE将数据从50维进一步降到2维，便于在平面上可视化。
5. 可视化：最后，使用Matplotlib生成散点图，不同的颜色表示不同的数字类别。



# 参考资料
