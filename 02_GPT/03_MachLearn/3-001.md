# 无监督学习

# 1. 主成分分析（PCA）

### 1. 协方差矩阵

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-155357.png" alt="Image Description" width="700">
</p>


### 2. 身高-体重举例

1. 绘制特征向量方向

在绘制特征向量时，我们需要将特征向量的方向和长度表示出来，使其在图形上可视化。特征向量方向表明了数据主要变异的方向，而特征向量的长度则与对应的特征值的平方根成正比，这样可以更直观地反映出每个方向上数据的分散程度。

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240617-104522.png" alt="Image Description" width="500">
</p>


- 特征向量的长度与特征值的关系

在上面的代码中，特征向量被缩放了 `2 * np.sqrt(eigenvalues[i])` 倍。这里的 2 是为了在图中更清晰地显示特征向量，而 `np.sqrt(eigenvalues[i])` 则是因为特征值的大小表示了在对应的特征向量方向上的数据方差。特征向量原本的长度是1（因为它们是单位向量），但我们通过乘以特征值的平方根来调整其长度，从而使得向量的长度在图形中反映出在该方向上的数据变异性大小。

- 特征向量的方向

特征向量的方向表示数据分布的主要方向。在二维空间中，每个特征向量都可以从数据中心指向一个特定方向。代码中使用 `plt.quiver` 函数绘制箭头来表示这些向量。`plt.quiver` 需要起点的 x, y 坐标（这里是数据的中心，`height_mean` 和 `weight_mean`），以及向量的 x, y 分量（`vec[0] 和 vec[1]`）。`angles='xy', scale_units='xy', scale=1` 这些参数确保向量按照 x, y 轴的比例正确绘制，而不是根据图的比例自动缩放。


```py
import numpy as np
import matplotlib.pyplot as plt

# 设置随机种子，以便每次运行代码时获得相同的结果
np.random.seed(0)

# 生成模拟数据
num_points = 100
height = np.random.normal(170, 10, num_points)  # 身高，均值170，标准差10
weight = height * 0.5 + np.random.normal(60, 5, num_points)  # 体重，与身高正相关

# 将数据中心化
height_mean = np.mean(height)
weight_mean = np.mean(weight)
height_centered = height - height_mean
weight_centered = weight - weight_mean

# 组装成一个二维数据集
data = np.vstack([height_centered, weight_centered]).T

# 计算协方巨阵
cov_matrix = np.cov(data, rowvar=False)
print("Covariance Matrix:\n", cov_matrix)

# 计算特征值和特征向量
eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
print("Eigenvalues before sorting:\n", eigenvalues)
print("Eigenvectors before sorting:\n", eigenvectors)

# 对特征值和特征向量排序（由小到大）
sorted_indices = np.argsort(eigenvalues)
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]

print("Sorted Eigenvalues:\n", eigenvalues)
print("Sorted Eigenvectors:\n", eigenvectors)

# 绘制原始数据
plt.scatter(height, weight, alpha=0.7)
plt.xlabel('Height (cm)')
plt.ylabel('Weight (kg)')
plt.title('Height vs Weight with Principal Components')

# 添加特征向量方向
for i in range(len(eigenvalues)):
    vec = eigenvectors[:, i] * 2 * np.sqrt(eigenvalues[i])  # 缩放特征向量以便可视化
    plt.quiver(height_mean, weight_mean, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, color=['r', 'b'])

plt.grid(True)
plt.axis('equal')
plt.show()
```

2. 绘制投影后的数据

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240617-105431.png" alt="Image Description" width="700">
</p>



解释

- 原始数据：散点图显示了身高和体重的原始分布。
- 降维后的数据：所有数据点都被投影到一个一维线上（实际上是选择的主成分方向）。这样，我们就捕获了数据中最主要的变异方向，而忽略了其他方向的小变异。

```py
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)
num_points = 100
height = np.random.normal(170, 10, num_points)
weight = height * 0.5 + np.random.normal(60, 5, num_points)

height_centered = height - np.mean(height)
weight_centered = weight - np.mean(weight)
data = np.vstack([height_centered, weight_centered]).T

cov_matrix = np.cov(data, rowvar=False)
eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

# 选择最大特征值对应的特征向量
max_eigenvector = eigenvectors[:, np.argmax(eigenvalues)]

# 投影数据到这个特征向量
projected_data = data @ max_eigenvector

# 绘制原始数据
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(height, weight, alpha=0.7)
plt.xlabel('Height (cm)')
plt.ylabel('Weight (kg)')
plt.title('Original Data')

# 绘制降维后的数据
plt.subplot(1, 2, 2)
plt.scatter(projected_data, np.zeros_like(projected_data), alpha=0.7)
plt.xlabel('Projected Data')
plt.title('Data After Dimension Reduction')
plt.yticks([])
plt.tight_layout()
plt.show()
```


### 3. 鸢尾花PCA

接下来，我将使用 `scikit-learn` 的鸢尾花数据集（Iris）作为示例，来展示如何使用PCA进行数据分析和可视化。这个数据集包含150个样本，每个样本有4个特征，分别是花萼长度、花萼宽度、花瓣长度和花瓣宽度。

1. 手动实现PCA代码

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.preprocessing import StandardScaler

# 加载数据
iris = datasets.load_iris()
X = iris.data

# 步骤 1: 数据标准化
scaler = StandardScaler()
X_std = scaler.fit_transform(X)
print("标准化后的数据：")
print(X_std[:5])  # 打印前5行数据

# 步骤 2: 计算协方差矩阵
cov_matrix = np.cov(X_std.T)
print("\n协方差矩阵：")
print(cov_matrix)

# 步骤 3: 计算协方差矩阵的特征值和特征向量
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
print("\n特征值：")
print(eigenvalues)
print("\n特征向量：")
print(eigenvectors)

# 步骤 4: 对特征向量排序，并选择主要特征向量
# 排序特征值并提取前两个最大的
indices = np.argsort(eigenvalues)[::-1]
eigenvalues_sorted = eigenvalues[indices]
eigenvectors_sorted = eigenvectors[:, indices]

print("\n排序后的特征值：")
print(eigenvalues_sorted)
print("\n排序后的特征向量：")
print(eigenvectors_sorted)

# 选择前两个特征向量
projection_matrix = eigenvectors_sorted[:, :2]
print("\n投影矩阵 (前两个特征向量)：")
print(projection_matrix)

# 步骤 5: 数据转换
X_pca = X_std.dot(projection_matrix)
print("\n降维后的数据：")
print(X_pca[:5])  # 打印前5行数据

# 可视化结果
plt.figure(figsize=(8, 6))
for i, target_name in zip([0, 1, 2], iris.target_names):
    plt.scatter(X_pca[iris.target == i, 0], X_pca[iris.target == i, 1], label=target_name)
plt.legend()
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of IRIS Dataset (Manual Steps)')
plt.show()
```

2. 使用sklearn的PCA类

```py
# 导入必要的库
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# 加载鸢尾花数据集
data = load_iris()
X = data.data
y = data.target
target_names = data.target_names

# 创建PCA对象，设定降维后的主成分数量为2
pca = PCA(n_components=2)

# 对数据进行降维
X_r = pca.fit_transform(X)

# 可视化
plt.figure()
colors = ['navy', 'turquoise', 'darkorange']
lw = 2

for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,
                label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('PCA of IRIS dataset')
plt.show()
```



### 4. 数据分析

1. 投影数据可视化

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-161210.png" alt="Image Description" width="450">
</p>

2. 数据分析

解释：

- **标准化后的数据**：对原始数据进行标准化，使每个特征的平均值为0，标准差为1。打印标准化后的数据的前5行。
- **协方差矩阵**：计算标准化数据的协方差矩阵，显示各个特征之间的协方差。
- **特征值和特征向量**：计算协方差矩阵的特征值和特征向量，并打印它们。
- **排序后的特征值和特征向量**：按照特征值的大小对特征值和特征向量进行排序，并打印排序结果。
- **投影矩阵**：选择前两个特征向量，构成投影矩阵，并打印出来。
- **降维后的数据**：使用投影矩阵将标准化后的数据转换到新的二维空间，并打印转换后的数据的前5行。


3. 过程数据

```
标准化后的数据：
[[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]
 [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]
 [-1.38535265  0.32841405 -1.39706395 -1.3154443 ]
 [-1.50652052  0.09821729 -1.2833891  -1.3154443 ]
 [-1.02184904  1.24920112 -1.34022653 -1.3154443 ]]

协方差矩阵：
[[ 1.00671141 -0.11835884  0.87760447  0.82343066]
 [-0.11835884  1.00671141 -0.43131554 -0.36858315]
 [ 0.87760447 -0.43131554  1.00671141  0.96932762]
 [ 0.82343066 -0.36858315  0.96932762  1.00671141]]

特征值：
[2.93808505 0.9201649  0.14774182 0.02085386]

特征向量：
[[ 0.52106591 -0.37741762 -0.71956635  0.26128628]
 [-0.26934744 -0.92329566  0.24438178 -0.12350962]
 [ 0.5804131  -0.02449161  0.14212637 -0.80144925]
 [ 0.56485654 -0.06694199  0.63427274  0.52359713]]

排序后的特征值：
[2.93808505 0.9201649  0.14774182 0.02085386]

排序后的特征向量：
[[ 0.52106591 -0.37741762 -0.71956635  0.26128628]
 [-0.26934744 -0.92329566  0.24438178 -0.12350962]
 [ 0.5804131  -0.02449161  0.14212637 -0.80144925]
 [ 0.56485654 -0.06694199  0.63427274  0.52359713]]

投影矩阵 (前两个特征向量)：
[[ 0.52106591 -0.37741762]
 [-0.26934744 -0.92329566]
 [ 0.5804131  -0.02449161]
 [ 0.56485654 -0.06694199]]

降维后的数据：
[[-2.26470281 -0.4800266 ]
 [-2.08096115  0.67413356]
 [-2.36422905  0.34190802]
 [-2.29938422  0.59739451]
 [-2.38984217 -0.64683538]]
```



# 2. 潜在语义分析 (LSA)

### 1. 基本概念

LSA 是一种基于矩阵分解的技术，通过对`词-文档`矩阵进行奇异值分解（SVD），将高维的文本数据映射到一个低维的潜在语义空间中。这种方法能够捕捉`词语和文档`之间的隐含关系，从而解决词义多样性和同义词问题。


### 2. 具体步骤

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-171135.png" alt="Image Description" width="700">
</p>

### 3. TF-IDF 特征矩阵

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-193619.png" alt="Image Description" width="700">
</p>



# 3. 非负矩阵分解

### 1. 基本概念

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-213002.png" alt="Image Description" width="700">
</p>

### 2. 具体步骤

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-194313.png" alt="Image Description" width="700">
</p>

### 3. 代码示例


```py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# 定义小型数据集
documents = [
    "The cat sat on the mat",
    "The dog sat on the log",
    "Cats and dogs are pets",
    "Dogs and cats are animals",
    "The mat and the log are objects"
]

# 使用TF-IDF向量化文本数据
vectorizer = TfidfVectorizer(stop_words='english')
X_tfidf = vectorizer.fit_transform(documents)

# 打印TF-IDF矩阵
print("TF-IDF 矩阵：")
print(pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names()))

# 使用NMF进行非负矩阵分解
n_components = 2
nmf_model = NMF(n_components=n_components, random_state=42)
W = nmf_model.fit_transform(X_tfidf)
H = nmf_model.components_

# 创建 DataFrame 以便查看
df_W = pd.DataFrame(W, columns=[f'Component {i+1}' for i in range(n_components)])
df_W['Document'] = documents

df_H = pd.DataFrame(H, columns=vectorizer.get_feature_names())
df_H.index = [f'Component {i+1}' for i in range(n_components)]

# 打印结果矩阵
print("\nW 矩阵：")
print(df_W)
print("\nH 矩阵：")
print(df_H)
```

- 代码分析

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240616-194720.png" alt="Image Description" width="700">
</p>

### 4. 输出结果

```
TF-IDF 矩阵：
    animals       cat      cats  ...   objects      pets       sat
0  0.000000  0.659118  0.000000  ...  0.000000  0.000000  0.531772
1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.531772
2  0.000000  0.000000  0.531772  ...  0.000000  0.659118  0.000000
3  0.659118  0.000000  0.531772  ...  0.000000  0.000000  0.000000
4  0.000000  0.000000  0.000000  ...  0.659118  0.000000  0.000000

[5 rows x 10 columns]

W 矩阵：
   Component 1  Component 2                         Document
0     0.645813     0.000000           The cat sat on the mat
1     0.645813     0.000000           The dog sat on the log
2     0.000000     0.790957           Cats and dogs are pets
3     0.000000     0.790957        Dogs and cats are animals
4     0.645813     0.000000  The mat and the log are objects

H 矩阵：
              animals     cat      cats  ...  objects      pets       sat
Component 1  0.000000  0.3402  0.000000  ...   0.3402  0.000000  0.548943
Component 2  0.416659  0.0000  0.672315  ...   0.0000  0.416659  0.000000

[2 rows x 10 columns]
```



# 参考资料
