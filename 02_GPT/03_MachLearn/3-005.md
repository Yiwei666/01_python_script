# 循环神经网络（RNN）

# 1. 序列数据

### 1. 序列数据

1. `序列数据`的定义
   - 序列数据是指多个样本之间存在顺序关系的数据类型。这与独立样本的处理不同，在序列数据中，每个样本的顺序对整个序列的理解和处理至关重要。

2. 序列数据的示例
   - 时间序列数据：如股票价格、天气数据等。
   - 语言数据：如句子中的单词序列。
   - 音频数据：如语音信号中的采样点序列。

3. 处理序列数据的挑战。处理序列数据的主要挑战在于如何捕捉和利用数据中存在的顺序关系。具体来说，挑战包括：
   - 捕捉长时间依赖关系：在长序列中，早期样本的信息可能对后期样本的处理很重要。
   - 上下文信息的保持：如何在处理每个样本时保留前面样本的信息，以便理解当前样本的含义。
   - 数据的顺序性：确保数据的顺序不会在处理过程中丢失或被打乱。


### 2. 序列数据处理

1. 全连接神经网络

在最简单的情况下，`全连接神经网络（FCN）`可以用于处理序列数据，但这种方法存在显著的局限性：

- FCN 无法有效捕捉序列中的长时间依赖关系，因为每个输入样本都被独立处理，没有记忆机制来保留前面的信息。

2. 循环神经网络（RNNs）

RNNs 是为处理序列数据而设计的一种神经网络架构。其核心思想是引入`状态（state）`的概念，使网络能够记住先前的输入信息，并利用这些信息来影响当前的输出。

- 状态：RNNs 通过`隐藏状态（hidden state）`来保持对序列中前面样本的信息的记忆。每次输入一个样本时，RNN 更新其隐藏状态，并将其传递到下一时间步。
- 训练方法：通过`反向传播算法（Backpropagation Through Time, BPTT）`来训练RNNs。这种方法可以处理序列中的时间依赖关系，但也可能遇到梯度消失或爆炸的问题。

3. 序列数据处理的应用

RNNs 在处理序列数据时表现出色，其应用包括但不限于：

- 语言模型：预测句子中下一个单词或字符。
- 机器翻译：将一句话从一种语言翻译到另一种语言。
- 语音识别：将音频信号转化为文本。


# 2. 序列数据全连接预测

### 1. 数据集和代码

1. 训练集和测试集

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-150928.png" alt="Image Description" width="700">
</p>

代码生成了一个更为复杂且具有随机性的时间序列数据集，从而使得全连接神经网络在训练集和测试集上的表现都变差了。实现这一结果的主要方法如下：

- 增加数据复杂性：叠加了多个不同周期和频率的正弦波，生成一个复杂的基础数据。
- 引入非周期性成分：加入了线性函数和指数函数，这些函数不会在数据中形成明显的周期性。
- 增加随机噪声：加入了较高水平的随机噪声，使得数据更加不可预测和不规则。


2. 代码

```py
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import MinMaxScaler

# Generating highly complex data with added noise and non-periodic components
def generate_highly_complex_data(samples=750, noise_level=0.5):
    x = np.linspace(0, 10 * np.pi, samples)
    y = np.sin(x) + 0.5 * np.sin(3 * x) + 0.25 * np.sin(5 * x) + 0.1 * np.sin(7 * x)
    y += 0.05 * np.sin(11 * x) + 0.1 * x + np.exp(0.01 * x) + np.random.normal(scale=noise_level, size=samples)
    # Adding non-linear, non-periodic component
    y += np.random.normal(scale=0.3, size=samples)
    return y

# Create training and testing datasets using a sliding window approach
def create_dataset(data, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)

# Generate highly complex data
data = generate_highly_complex_data()
scaler = MinMaxScaler(feature_range=(0, 1))
data = scaler.fit_transform(data.reshape(-1, 1)).flatten()

# Define window size
window_size = 20

# Create training and testing datasets
train_size = 500
X_train, y_train = create_dataset(data[:train_size], window_size)
X_test, y_test = create_dataset(data[train_size:], window_size)

# Build the neural network model
model = Sequential()
model.add(Dense(20, input_dim=window_size, activation='relu'))
model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)

# Make predictions
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# Invert predictions
train_predict = scaler.inverse_transform(train_predict)
y_train = scaler.inverse_transform([y_train])
test_predict = scaler.inverse_transform(test_predict)
y_test = scaler.inverse_transform([y_test])

# Plot results
plt.figure(figsize=(14, 7))
plt.subplot(1, 2, 1)
plt.plot(y_train[0], label='Actual')
plt.plot(train_predict, label='Prediction')
plt.title('Train Data')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(y_test[0], label='Actual')
plt.plot(test_predict, label='Prediction')
plt.title('Test Data')
plt.legend()

plt.show()
```

### 2. 训练过程

上述代码的功能是生成一个高度复杂且具有随机噪声的时间序列数据集，并使用全连接神经网络对其进行训练和预测。具体步骤和功能如下：

1. 导入必要的库：
   - `numpy`用于数值计算。
   - `matplotlib.pyplot`用于绘图。
   - `tensorflow.keras`用于构建和训练神经网络。
   - `sklearn.preprocessing`中的`MinMaxScaler`用于数据标准化。

2. 生成复杂数据：

   - `generate_highly_complex_data`函数生成一个复杂的时间序列数据，其中包含多个不同频率的正弦波、线性和指数函数，以及大量的随机噪声。
   - 通过叠加这些成分，数据变得高度不规则和难以预测。

3. 创建数据集：

   - `create_dataset`函数使用滑动窗口方法从时间序列数据中创建特征和标签数据集。
   - 输入窗口大小为20，意味着每个样本包含20个数据点，目标是预测第21个数据点。

4. 数据标准化：

   - 使用`MinMaxScaler`将数据标准化到`[0, 1]`范围内，这对神经网络训练有帮助。

5. 构建训练和测试数据集：

   - 将前500个样本作为训练集，剩余的样本作为测试集。

6. 构建神经网络模型：

   - 使用`Sequential模型`构建一个全连接神经网络。
   - 模型包含一个隐藏层，包含20个神经元，使用`ReLU`激活函数，以及一个输出层。

7. 编译和训练模型：

   - 使用`均方误差（MSE）`作为损失函数，`Adam优化器`进行优化。
   - 在训练数据上训练模型50个周期，每批次10个样本。

8. 进行预测：

   - 使用训练好的模型对训练集和测试集进行预测。
   - 将预测结果反标准化，转换回原始数据范围。

9. 绘制结果：

   - 绘制训练数据和测试数据的实际值和预测值，以可视化神经网络的表现。
   - 左图为训练数据，右图为测试数据。
   - 通过上述步骤，该代码展示了全连接神经网络在处理复杂和高度随机的时间序列数据时的表现。由于数据的高度复杂性和随机性，神经网络在训练集和测试集上的预测表现较差，这也验证了全连接神经网络在处理高度不规则时间序列数据时的局限性。


### 3. 常见梯度下降方法

1. 梯度下降算法

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-153117.png" alt="Image Description" width="700">
</p>

2. 相关代码

```py
# 编译和训练模型
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)
```

3. 这段代码说明了：
   - `epochs=50`：模型将遍历整个训练数据集`50次`，每次称为一个`周期`。
   - `batch_size=10`：在每个周期中，训练数据集被分成若干个`批次`，每个批次包含10个样本。模型在每个批次上计算平均损失并更新权重。
   - 因此，在一个周期内，模型会遍历`所有样本`，每`10个样本`更新一次权重。


### 4. 周期数和批次数

1. 定义

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-155021.png" alt="Image Description" width="700">
</p>

2. 周期数和批次述对训练集和测试集的表现

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-155332.png" alt="Image Description" width="700">
</p>

3. 训练和绘图代码

```py
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import MinMaxScaler

# Generating data
def generate_data(samples=750, noise_level=0.5):
    x = np.linspace(0, 10 * np.pi, samples)
    y = np.sin(x) + 0.5 * np.sin(3 * x) + 0.25 * np.sin(5 * x) + 0.1 * np.sin(7 * x)
    y += 0.05 * np.sin(11 * x) + 0.1 * x + np.exp(0.01 * x) + np.random.normal(scale=noise_level, size=samples)
    y += np.random.normal(scale=0.3, size=samples)
    return y

# Create dataset
def create_dataset(data, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)

# Generate data
data = generate_data()
scaler = MinMaxScaler(feature_range=(0, 1))
data = scaler.fit_transform(data.reshape(-1, 1)).flatten()

# Define window size
window_size = 20
train_size = 500

X_train, y_train = create_dataset(data[:train_size], window_size)
X_test, y_test = create_dataset(data[train_size:], window_size)

# Function to build and train model
def build_and_train_model(epochs, batch_size):
    model = Sequential()
    model.add(Dense(20, input_dim=window_size, activation='relu'))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
    return model

# Different configurations
configs = [
    (50, 10),
    (100, 10),
    (50, 32),
    (100, 32)
]

# Plot results
plt.figure(figsize=(14, 14))
for i, (epochs, batch_size) in enumerate(configs):
    model = build_and_train_model(epochs, batch_size)
    train_predict = model.predict(X_train)
    test_predict = model.predict(X_test)
    train_predict = scaler.inverse_transform(train_predict)
    y_train_inv = scaler.inverse_transform([y_train])
    test_predict = scaler.inverse_transform(test_predict)
    y_test_inv = scaler.inverse_transform([y_test])
    
    plt.subplot(4, 2, 2*i+1)
    plt.plot(y_train_inv[0], label='Actual')
    plt.plot(train_predict, label='Prediction')
    plt.title(f'Train Data (Epochs: {epochs}, Batch Size: {batch_size})')
    plt.legend()
    
    plt.subplot(4, 2, 2*i+2)
    plt.plot(y_test_inv[0], label='Actual')
    plt.plot(test_predict, label='Prediction')
    plt.title(f'Test Data (Epochs: {epochs}, Batch Size: {batch_size})')
    plt.legend()

plt.show()
```

### 5. 滑动窗口法与数据集创建

1. 示意图

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-161408.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-160502.png" alt="Image Description" width="600">
</p>

2. 窗口大小确定

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-161146.png" alt="Image Description" width="700">
</p>

3. 滑动窗口法优点

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-161306.png" alt="Image Description" width="700">
</p>

# 3. 循环神经网络（Recurrent Neural Networks，RNNs）

### 1. RNN计算图

`循环神经网络（Recurrent Neural Networks, RNNs）`是一类设计用来识别序列数据模式的神经网络，如文本、基因组、手写文字、语音语言和传感器、股市以及政府机构提供的数值时间序列数据。与传统的前馈神经网络不同，RNNs具有形成有向循环的连接，使其能够保持之前输入的状态或记忆。这一特性使它们在输入顺序重要的任务中非常强大。

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-102637.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-103313.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-103507.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-103621.png" alt="Image Description" width="700">
</p>

### 2. RNN设计模式

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240706-184745.png" alt="Image Description" width="700">
</p>

- 在每个时间步产生输出并且仅有从一个时间步的输出到下一个时间步的隐藏单元的循环连接的RNN

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240706-184857.png" alt="Image Description" width="700">
</p>

- 读取整个序列然后生成单个输出的RNN

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240706-184946.png" alt="Image Description" width="700">
</p>


# 4. 双向循环神经网络（Bidirectional RNNs，BRNNs）



# 5. 深度循环网络（Deep Recurrent Networks）



# 6. 递归神经网络（Recursive Neural Networks）



# 7. 长短期记忆网络 (LSTM)




# 8. 门控循环单元 (GRU)




# 参考资料

1. "Deep Learning: A Visual Approach" by Andrew Glassner
2. "Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville






