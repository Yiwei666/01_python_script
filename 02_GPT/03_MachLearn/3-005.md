# å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰

# 1. åºåˆ—æ•°æ®

### 1. åºåˆ—æ•°æ®

1. `åºåˆ—æ•°æ®`çš„å®šä¹‰
   - åºåˆ—æ•°æ®æ˜¯æŒ‡å¤šä¸ªæ ·æœ¬ä¹‹é—´å­˜åœ¨é¡ºåºå…³ç³»çš„æ•°æ®ç±»å‹ã€‚è¿™ä¸ç‹¬ç«‹æ ·æœ¬çš„å¤„ç†ä¸åŒï¼Œåœ¨åºåˆ—æ•°æ®ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬çš„é¡ºåºå¯¹æ•´ä¸ªåºåˆ—çš„ç†è§£å’Œå¤„ç†è‡³å…³é‡è¦ã€‚

2. åºåˆ—æ•°æ®çš„ç¤ºä¾‹
   - æ—¶é—´åºåˆ—æ•°æ®ï¼šå¦‚è‚¡ç¥¨ä»·æ ¼ã€å¤©æ°”æ•°æ®ç­‰ã€‚
   - è¯­è¨€æ•°æ®ï¼šå¦‚å¥å­ä¸­çš„å•è¯åºåˆ—ã€‚
   - éŸ³é¢‘æ•°æ®ï¼šå¦‚è¯­éŸ³ä¿¡å·ä¸­çš„é‡‡æ ·ç‚¹åºåˆ—ã€‚

3. å¤„ç†åºåˆ—æ•°æ®çš„æŒ‘æˆ˜ã€‚å¤„ç†åºåˆ—æ•°æ®çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•æ•æ‰å’Œåˆ©ç”¨æ•°æ®ä¸­å­˜åœ¨çš„é¡ºåºå…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼ŒæŒ‘æˆ˜åŒ…æ‹¬ï¼š
   - æ•æ‰é•¿æ—¶é—´ä¾èµ–å…³ç³»ï¼šåœ¨é•¿åºåˆ—ä¸­ï¼Œæ—©æœŸæ ·æœ¬çš„ä¿¡æ¯å¯èƒ½å¯¹åæœŸæ ·æœ¬çš„å¤„ç†å¾ˆé‡è¦ã€‚
   - ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä¿æŒï¼šå¦‚ä½•åœ¨å¤„ç†æ¯ä¸ªæ ·æœ¬æ—¶ä¿ç•™å‰é¢æ ·æœ¬çš„ä¿¡æ¯ï¼Œä»¥ä¾¿ç†è§£å½“å‰æ ·æœ¬çš„å«ä¹‰ã€‚
   - æ•°æ®çš„é¡ºåºæ€§ï¼šç¡®ä¿æ•°æ®çš„é¡ºåºä¸ä¼šåœ¨å¤„ç†è¿‡ç¨‹ä¸­ä¸¢å¤±æˆ–è¢«æ‰“ä¹±ã€‚


### 2. åºåˆ—æ•°æ®å¤„ç†

1. å…¨è¿æ¥ç¥ç»ç½‘ç»œ

åœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œ`å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆFCNï¼‰`å¯ä»¥ç”¨äºå¤„ç†åºåˆ—æ•°æ®ï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼š

- FCN æ— æ³•æœ‰æ•ˆæ•æ‰åºåˆ—ä¸­çš„é•¿æ—¶é—´ä¾èµ–å…³ç³»ï¼Œå› ä¸ºæ¯ä¸ªè¾“å…¥æ ·æœ¬éƒ½è¢«ç‹¬ç«‹å¤„ç†ï¼Œæ²¡æœ‰è®°å¿†æœºåˆ¶æ¥ä¿ç•™å‰é¢çš„ä¿¡æ¯ã€‚

2. å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰

RNNs æ˜¯ä¸ºå¤„ç†åºåˆ—æ•°æ®è€Œè®¾è®¡çš„ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å¼•å…¥`çŠ¶æ€ï¼ˆstateï¼‰`çš„æ¦‚å¿µï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿè®°ä½å…ˆå‰çš„è¾“å…¥ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥å½±å“å½“å‰çš„è¾“å‡ºã€‚

- çŠ¶æ€ï¼šRNNs é€šè¿‡`éšè—çŠ¶æ€ï¼ˆhidden stateï¼‰`æ¥ä¿æŒå¯¹åºåˆ—ä¸­å‰é¢æ ·æœ¬çš„ä¿¡æ¯çš„è®°å¿†ã€‚æ¯æ¬¡è¾“å…¥ä¸€ä¸ªæ ·æœ¬æ—¶ï¼ŒRNN æ›´æ–°å…¶éšè—çŠ¶æ€ï¼Œå¹¶å°†å…¶ä¼ é€’åˆ°ä¸‹ä¸€æ—¶é—´æ­¥ã€‚
- è®­ç»ƒæ–¹æ³•ï¼šé€šè¿‡`åå‘ä¼ æ’­ç®—æ³•ï¼ˆBackpropagation Through Time, BPTTï¼‰`æ¥è®­ç»ƒRNNsã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¤„ç†åºåˆ—ä¸­çš„æ—¶é—´ä¾èµ–å…³ç³»ï¼Œä½†ä¹Ÿå¯èƒ½é‡åˆ°æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„é—®é¢˜ã€‚

3. åºåˆ—æ•°æ®å¤„ç†çš„åº”ç”¨

RNNs åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶è¡¨ç°å‡ºè‰²ï¼Œå…¶åº”ç”¨åŒ…æ‹¬ä½†ä¸é™äºï¼š

- è¯­è¨€æ¨¡å‹ï¼šé¢„æµ‹å¥å­ä¸­ä¸‹ä¸€ä¸ªå•è¯æˆ–å­—ç¬¦ã€‚
- æœºå™¨ç¿»è¯‘ï¼šå°†ä¸€å¥è¯ä»ä¸€ç§è¯­è¨€ç¿»è¯‘åˆ°å¦ä¸€ç§è¯­è¨€ã€‚
- è¯­éŸ³è¯†åˆ«ï¼šå°†éŸ³é¢‘ä¿¡å·è½¬åŒ–ä¸ºæ–‡æœ¬ã€‚


# 2. åºåˆ—æ•°æ®å…¨è¿æ¥é¢„æµ‹

### 1. æ•°æ®é›†å’Œä»£ç 

1. è®­ç»ƒé›†å’Œæµ‹è¯•é›†

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-150928.png" alt="Image Description" width="700">
</p>

ä»£ç ç”Ÿæˆäº†ä¸€ä¸ªæ›´ä¸ºå¤æ‚ä¸”å…·æœ‰éšæœºæ€§çš„æ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œä»è€Œä½¿å¾—å…¨è¿æ¥ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„è¡¨ç°éƒ½å˜å·®äº†ã€‚å®ç°è¿™ä¸€ç»“æœçš„ä¸»è¦æ–¹æ³•å¦‚ä¸‹ï¼š

- å¢åŠ æ•°æ®å¤æ‚æ€§ï¼šå åŠ äº†å¤šä¸ªä¸åŒå‘¨æœŸå’Œé¢‘ç‡çš„æ­£å¼¦æ³¢ï¼Œç”Ÿæˆä¸€ä¸ªå¤æ‚çš„åŸºç¡€æ•°æ®ã€‚
- å¼•å…¥éå‘¨æœŸæ€§æˆåˆ†ï¼šåŠ å…¥äº†çº¿æ€§å‡½æ•°å’ŒæŒ‡æ•°å‡½æ•°ï¼Œè¿™äº›å‡½æ•°ä¸ä¼šåœ¨æ•°æ®ä¸­å½¢æˆæ˜æ˜¾çš„å‘¨æœŸæ€§ã€‚
- å¢åŠ éšæœºå™ªå£°ï¼šåŠ å…¥äº†è¾ƒé«˜æ°´å¹³çš„éšæœºå™ªå£°ï¼Œä½¿å¾—æ•°æ®æ›´åŠ ä¸å¯é¢„æµ‹å’Œä¸è§„åˆ™ã€‚


2. ä»£ç 

```py
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import MinMaxScaler

# Generating highly complex data with added noise and non-periodic components
def generate_highly_complex_data(samples=750, noise_level=0.5):
    x = np.linspace(0, 10 * np.pi, samples)
    y = np.sin(x) + 0.5 * np.sin(3 * x) + 0.25 * np.sin(5 * x) + 0.1 * np.sin(7 * x)
    y += 0.05 * np.sin(11 * x) + 0.1 * x + np.exp(0.01 * x) + np.random.normal(scale=noise_level, size=samples)
    # Adding non-linear, non-periodic component
    y += np.random.normal(scale=0.3, size=samples)
    return y

# Create training and testing datasets using a sliding window approach
def create_dataset(data, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)

# Generate highly complex data
data = generate_highly_complex_data()
scaler = MinMaxScaler(feature_range=(0, 1))
data = scaler.fit_transform(data.reshape(-1, 1)).flatten()

# Define window size
window_size = 20

# Create training and testing datasets
train_size = 500
X_train, y_train = create_dataset(data[:train_size], window_size)
X_test, y_test = create_dataset(data[train_size:], window_size)

# Build the neural network model
model = Sequential()
model.add(Dense(20, input_dim=window_size, activation='relu'))
model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)

# Make predictions
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# Invert predictions
train_predict = scaler.inverse_transform(train_predict)
y_train = scaler.inverse_transform([y_train])
test_predict = scaler.inverse_transform(test_predict)
y_test = scaler.inverse_transform([y_test])

# Plot results
plt.figure(figsize=(14, 7))
plt.subplot(1, 2, 1)
plt.plot(y_train[0], label='Actual')
plt.plot(train_predict, label='Prediction')
plt.title('Train Data')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(y_test[0], label='Actual')
plt.plot(test_predict, label='Prediction')
plt.title('Test Data')
plt.legend()

plt.show()
```

### 2. è®­ç»ƒè¿‡ç¨‹

ä¸Šè¿°ä»£ç çš„åŠŸèƒ½æ˜¯ç”Ÿæˆä¸€ä¸ªé«˜åº¦å¤æ‚ä¸”å…·æœ‰éšæœºå™ªå£°çš„æ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨å…¨è¿æ¥ç¥ç»ç½‘ç»œå¯¹å…¶è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚å…·ä½“æ­¥éª¤å’ŒåŠŸèƒ½å¦‚ä¸‹ï¼š

1. å¯¼å…¥å¿…è¦çš„åº“ï¼š
   - `numpy`ç”¨äºæ•°å€¼è®¡ç®—ã€‚
   - `matplotlib.pyplot`ç”¨äºç»˜å›¾ã€‚
   - `tensorflow.keras`ç”¨äºæ„å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œã€‚
   - `sklearn.preprocessing`ä¸­çš„`MinMaxScaler`ç”¨äºæ•°æ®æ ‡å‡†åŒ–ã€‚

2. ç”Ÿæˆå¤æ‚æ•°æ®ï¼š

   - `generate_highly_complex_data`å‡½æ•°ç”Ÿæˆä¸€ä¸ªå¤æ‚çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œå…¶ä¸­åŒ…å«å¤šä¸ªä¸åŒé¢‘ç‡çš„æ­£å¼¦æ³¢ã€çº¿æ€§å’ŒæŒ‡æ•°å‡½æ•°ï¼Œä»¥åŠå¤§é‡çš„éšæœºå™ªå£°ã€‚
   - é€šè¿‡å åŠ è¿™äº›æˆåˆ†ï¼Œæ•°æ®å˜å¾—é«˜åº¦ä¸è§„åˆ™å’Œéš¾ä»¥é¢„æµ‹ã€‚

3. åˆ›å»ºæ•°æ®é›†ï¼š

   - `create_dataset`å‡½æ•°ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•ä»æ—¶é—´åºåˆ—æ•°æ®ä¸­åˆ›å»ºç‰¹å¾å’Œæ ‡ç­¾æ•°æ®é›†ã€‚
   - è¾“å…¥çª—å£å¤§å°ä¸º20ï¼Œæ„å‘³ç€æ¯ä¸ªæ ·æœ¬åŒ…å«20ä¸ªæ•°æ®ç‚¹ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ç¬¬21ä¸ªæ•°æ®ç‚¹ã€‚

4. æ•°æ®æ ‡å‡†åŒ–ï¼š

   - ä½¿ç”¨`MinMaxScaler`å°†æ•°æ®æ ‡å‡†åŒ–åˆ°`[0, 1]`èŒƒå›´å†…ï¼Œè¿™å¯¹ç¥ç»ç½‘ç»œè®­ç»ƒæœ‰å¸®åŠ©ã€‚

5. æ„å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ï¼š

   - å°†å‰500ä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä½™çš„æ ·æœ¬ä½œä¸ºæµ‹è¯•é›†ã€‚

6. æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ï¼š

   - ä½¿ç”¨`Sequentialæ¨¡å‹`æ„å»ºä¸€ä¸ªå…¨è¿æ¥ç¥ç»ç½‘ç»œã€‚
   - æ¨¡å‹åŒ…å«ä¸€ä¸ªéšè—å±‚ï¼ŒåŒ…å«20ä¸ªç¥ç»å…ƒï¼Œä½¿ç”¨`ReLU`æ¿€æ´»å‡½æ•°ï¼Œä»¥åŠä¸€ä¸ªè¾“å‡ºå±‚ã€‚

7. ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹ï¼š

   - ä½¿ç”¨`å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰`ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œ`Adamä¼˜åŒ–å™¨`è¿›è¡Œä¼˜åŒ–ã€‚
   - åœ¨è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹50ä¸ªå‘¨æœŸï¼Œæ¯æ‰¹æ¬¡10ä¸ªæ ·æœ¬ã€‚

8. è¿›è¡Œé¢„æµ‹ï¼š

   - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚
   - å°†é¢„æµ‹ç»“æœåæ ‡å‡†åŒ–ï¼Œè½¬æ¢å›åŸå§‹æ•°æ®èŒƒå›´ã€‚

9. ç»˜åˆ¶ç»“æœï¼š

   - ç»˜åˆ¶è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„å®é™…å€¼å’Œé¢„æµ‹å€¼ï¼Œä»¥å¯è§†åŒ–ç¥ç»ç½‘ç»œçš„è¡¨ç°ã€‚
   - å·¦å›¾ä¸ºè®­ç»ƒæ•°æ®ï¼Œå³å›¾ä¸ºæµ‹è¯•æ•°æ®ã€‚
   - é€šè¿‡ä¸Šè¿°æ­¥éª¤ï¼Œè¯¥ä»£ç å±•ç¤ºäº†å…¨è¿æ¥ç¥ç»ç½‘ç»œåœ¨å¤„ç†å¤æ‚å’Œé«˜åº¦éšæœºçš„æ—¶é—´åºåˆ—æ•°æ®æ—¶çš„è¡¨ç°ã€‚ç”±äºæ•°æ®çš„é«˜åº¦å¤æ‚æ€§å’Œéšæœºæ€§ï¼Œç¥ç»ç½‘ç»œåœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„é¢„æµ‹è¡¨ç°è¾ƒå·®ï¼Œè¿™ä¹ŸéªŒè¯äº†å…¨è¿æ¥ç¥ç»ç½‘ç»œåœ¨å¤„ç†é«˜åº¦ä¸è§„åˆ™æ—¶é—´åºåˆ—æ•°æ®æ—¶çš„å±€é™æ€§ã€‚


### 3. å¸¸è§æ¢¯åº¦ä¸‹é™æ–¹æ³•

1. æ¢¯åº¦ä¸‹é™ç®—æ³•

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-153117.png" alt="Image Description" width="700">
</p>

2. ç›¸å…³ä»£ç 

```py
# ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)
```

3. è¿™æ®µä»£ç è¯´æ˜äº†ï¼š
   - `epochs=50`ï¼šæ¨¡å‹å°†éå†æ•´ä¸ªè®­ç»ƒæ•°æ®é›†`50æ¬¡`ï¼Œæ¯æ¬¡ç§°ä¸ºä¸€ä¸ª`å‘¨æœŸ`ã€‚
   - `batch_size=10`ï¼šåœ¨æ¯ä¸ªå‘¨æœŸä¸­ï¼Œè®­ç»ƒæ•°æ®é›†è¢«åˆ†æˆè‹¥å¹²ä¸ª`æ‰¹æ¬¡`ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«10ä¸ªæ ·æœ¬ã€‚æ¨¡å‹åœ¨æ¯ä¸ªæ‰¹æ¬¡ä¸Šè®¡ç®—å¹³å‡æŸå¤±å¹¶æ›´æ–°æƒé‡ã€‚
   - å› æ­¤ï¼Œåœ¨ä¸€ä¸ªå‘¨æœŸå†…ï¼Œæ¨¡å‹ä¼šéå†`æ‰€æœ‰æ ·æœ¬`ï¼Œæ¯`10ä¸ªæ ·æœ¬`æ›´æ–°ä¸€æ¬¡æƒé‡ã€‚


### 4. å‘¨æœŸæ•°å’Œæ‰¹æ¬¡æ•°

1. å®šä¹‰

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-155021.png" alt="Image Description" width="700">
</p>

2. å‘¨æœŸæ•°å’Œæ‰¹æ¬¡è¿°å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„è¡¨ç°

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-155332.png" alt="Image Description" width="700">
</p>

3. è®­ç»ƒå’Œç»˜å›¾ä»£ç 

```py
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import MinMaxScaler

# Generating data
def generate_data(samples=750, noise_level=0.5):
    x = np.linspace(0, 10 * np.pi, samples)
    y = np.sin(x) + 0.5 * np.sin(3 * x) + 0.25 * np.sin(5 * x) + 0.1 * np.sin(7 * x)
    y += 0.05 * np.sin(11 * x) + 0.1 * x + np.exp(0.01 * x) + np.random.normal(scale=noise_level, size=samples)
    y += np.random.normal(scale=0.3, size=samples)
    return y

# Create dataset
def create_dataset(data, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)

# Generate data
data = generate_data()
scaler = MinMaxScaler(feature_range=(0, 1))
data = scaler.fit_transform(data.reshape(-1, 1)).flatten()

# Define window size
window_size = 20
train_size = 500

X_train, y_train = create_dataset(data[:train_size], window_size)
X_test, y_test = create_dataset(data[train_size:], window_size)

# Function to build and train model
def build_and_train_model(epochs, batch_size):
    model = Sequential()
    model.add(Dense(20, input_dim=window_size, activation='relu'))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
    return model

# Different configurations
configs = [
    (50, 10),
    (100, 10),
    (50, 32),
    (100, 32)
]

# Plot results
plt.figure(figsize=(14, 14))
for i, (epochs, batch_size) in enumerate(configs):
    model = build_and_train_model(epochs, batch_size)
    train_predict = model.predict(X_train)
    test_predict = model.predict(X_test)
    train_predict = scaler.inverse_transform(train_predict)
    y_train_inv = scaler.inverse_transform([y_train])
    test_predict = scaler.inverse_transform(test_predict)
    y_test_inv = scaler.inverse_transform([y_test])
    
    plt.subplot(4, 2, 2*i+1)
    plt.plot(y_train_inv[0], label='Actual')
    plt.plot(train_predict, label='Prediction')
    plt.title(f'Train Data (Epochs: {epochs}, Batch Size: {batch_size})')
    plt.legend()
    
    plt.subplot(4, 2, 2*i+2)
    plt.plot(y_test_inv[0], label='Actual')
    plt.plot(test_predict, label='Prediction')
    plt.title(f'Test Data (Epochs: {epochs}, Batch Size: {batch_size})')
    plt.legend()

plt.show()
```

### 5. æ»‘åŠ¨çª—å£æ³•ä¸æ•°æ®é›†åˆ›å»º

1. ç¤ºæ„å›¾

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-161408.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-160502.png" alt="Image Description" width="600">
</p>

2. çª—å£å¤§å°ç¡®å®š

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-161146.png" alt="Image Description" width="700">
</p>

3. æ»‘åŠ¨çª—å£æ³•ä¼˜ç‚¹

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-161306.png" alt="Image Description" width="700">
</p>

# 3. å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networksï¼ŒRNNsï¼‰

### 1. RNNè®¡ç®—å›¾

`å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networks, RNNsï¼‰`æ˜¯ä¸€ç±»è®¾è®¡ç”¨æ¥è¯†åˆ«åºåˆ—æ•°æ®æ¨¡å¼çš„ç¥ç»ç½‘ç»œï¼Œå¦‚æ–‡æœ¬ã€åŸºå› ç»„ã€æ‰‹å†™æ–‡å­—ã€è¯­éŸ³è¯­è¨€å’Œä¼ æ„Ÿå™¨ã€è‚¡å¸‚ä»¥åŠæ”¿åºœæœºæ„æä¾›çš„æ•°å€¼æ—¶é—´åºåˆ—æ•°æ®ã€‚ä¸ä¼ ç»Ÿçš„å‰é¦ˆç¥ç»ç½‘ç»œä¸åŒï¼ŒRNNså…·æœ‰å½¢æˆæœ‰å‘å¾ªç¯çš„è¿æ¥ï¼Œä½¿å…¶èƒ½å¤Ÿä¿æŒä¹‹å‰è¾“å…¥çš„çŠ¶æ€æˆ–è®°å¿†ã€‚è¿™ä¸€ç‰¹æ€§ä½¿å®ƒä»¬åœ¨è¾“å…¥é¡ºåºé‡è¦çš„ä»»åŠ¡ä¸­éå¸¸å¼ºå¤§ã€‚

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-102637.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-103313.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-103507.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-103621.png" alt="Image Description" width="700">
</p>

### 2. RNNè®¾è®¡æ¨¡å¼

ğŸ’ åœ¨æ¯ä¸ªæ—¶é—´æ­¥äº§ç”Ÿè¾“å‡ºå¹¶ä¸”ä»…æœ‰ä»ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºåˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—å•å…ƒçš„å¾ªç¯è¿æ¥çš„RNN

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240706-184857.png" alt="Image Description" width="700">
</p>

ğŸ’ è¯»å–æ•´ä¸ªåºåˆ—ç„¶åç”Ÿæˆå•ä¸ªè¾“å‡ºçš„RNN

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240706-184946.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240706-185844.png" alt="Image Description" width="700">
</p>

ğŸ’ ä¸‰ç§è®¾è®¡æ¨¡å¼å¯¹æ¯”

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240706-184745.png" alt="Image Description" width="700">
</p>

1. å…³é”®åŒºåˆ«
   - å‰ä¸¤ç§è®¾è®¡æ¨¡å¼ï¼šæ¯ä¸ªæ—¶é—´æ­¥éƒ½ä¼šç”Ÿæˆä¸€ä¸ªè¾“å‡ºï¼Œå¹¶è®¡ç®—ä¸€ä¸ªç›¸åº”çš„æŸå¤±å‡½æ•°å€¼ã€‚è¿™æ„å‘³ç€ä¸€æ¬¡å‰å‘ä¼ æ’­ä¼šäº§ç”Ÿå¤šä¸ªæŸå¤±å‡½æ•°å€¼ï¼Œæ‰€æœ‰è¿™äº›æŸå¤±å‡½æ•°å€¼çš„å’Œæˆ–å¹³å‡å€¼å°†ç”¨äºåå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ã€‚
   - ç¬¬ä¸‰ç§è®¾è®¡æ¨¡å¼ï¼šæ•´ä¸ªåºåˆ—å¤„ç†å®Œæˆååªç”Ÿæˆä¸€ä¸ªæœ€ç»ˆè¾“å‡ºï¼Œå¹¶è®¡ç®—ä¸€ä¸ªå•ä¸€çš„æŸå¤±å‡½æ•°å€¼ã€‚è¿™æ„å‘³ç€ä¸€æ¬¡å‰å‘ä¼ æ’­åªä¼šäº§ç”Ÿä¸€ä¸ªæŸå¤±å‡½æ•°å€¼ã€‚

2. è¿™ç§å·®å¼‚åæ˜ äº†å®ƒä»¬åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸­çš„é€‚ç”¨æ€§ï¼š
   - å‰ä¸¤ç§è®¾è®¡æ¨¡å¼é€‚ç”¨äºéœ€è¦å¯¹æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºè¿›è¡Œç›‘ç£å­¦ä¹ çš„ä»»åŠ¡ï¼Œå¦‚æ—¶é—´åºåˆ—é¢„æµ‹å’Œåºåˆ—æ ‡æ³¨ã€‚
   - ç¬¬ä¸‰ç§è®¾è®¡æ¨¡å¼é€‚ç”¨äºéœ€è¦å¯¹æ•´ä¸ªåºåˆ—è¿›è¡Œæ€»ç»“æˆ–åˆ†ç±»çš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»å’Œåºåˆ—æ€»ç»“ã€‚


# 4. åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆBidirectional RNNsï¼ŒBRNNsï¼‰



# 5. æ·±åº¦å¾ªç¯ç½‘ç»œï¼ˆDeep Recurrent Networksï¼‰



# 6. é€’å½’ç¥ç»ç½‘ç»œï¼ˆRecursive Neural Networksï¼‰



# 7. é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (LSTM)




# 8. é—¨æ§å¾ªç¯å•å…ƒ (GRU)




# å‚è€ƒèµ„æ–™

1. "Deep Learning: A Visual Approach" by Andrew Glassner
2. "Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville






