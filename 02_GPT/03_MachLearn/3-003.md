# 有监督学习

# 1. 线性回归

### 1. 线性回归定义及数学表达

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240928-213231.png" alt="Image Description" width="700">
</p>


### 2. 训练过程数据点的使用

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240928-213432.png" alt="Image Description" width="700">
</p>


### 3. 训练结束的判断条件

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240928-213558.png" alt="Image Description" width="700">
</p>




# 2. 逻辑回归



# 3. 决策树



# 4. 支持向量机

### 1. 核函数

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240626-150530.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240626-150644.png" alt="Image Description" width="700">
</p>

### 2. 内积

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240626-150918.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240626-152247.png" alt="Image Description" width="700">
</p>

### 3. 核函数分类

线性核、多项式核、径向基函数核、Sigmoid核

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240626-152526.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240626-152753.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240626-152837.png" alt="Image Description" width="700">
</p>

### 4. 鸢尾花SVM

对于 Iris 数据集，我们可以将四个特征降维到两个主成分，然后使用这两个主成分训练 SVM 模型并可视化。这样，我们可以直观地看到 SVM 如何在经过 PCA 降维后的数据上使用 RBF 核进行分类。

- 决策边界可视化

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240626-154617.png" alt="Image Description" width="450">
</p>

- 源码

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 加载 Iris 数据集
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 标准化特征数据
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 应用 PCA 降维到 2 维
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 分割数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)

# 创建 SVM 分类器，使用 RBF 核
svm = SVC(kernel='rbf', gamma='auto', C=1.0)

# 训练模型
svm.fit(X_train, y_train)

# 创建网格以绘制图形
x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))

# 预测整个网格
Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# 绘制结果
plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('SVM with RBF Kernel on PCA-reduced Iris Dataset')
plt.show()
```



# 2. 有监督学习重要概念

🔹 scikit-learn用户手册：https://scikit-learn.org/stable/user_guide.html


# 1. 线性回归

1. **线性回归**：线性回归（linear regression）是用于预测回归问题的算法。该算法不难理解，算法中根据训练数据计算使损失最小的参数的做法是有监督学习算法的共同之处。

- 线性回归数学模型

$$
h_\theta(X) = \theta_0 + \theta_1 X_1 + \theta_2 X_2 + \ldots + \theta_n X_n
$$

- 均方误差损失函数

$$
J(\theta_0, \theta_1, \ldots, \theta_n) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(X_i) - y_i)^2
$$

<p align="center">
  <img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_001.png" alt="Image Description" width="400">
</p>

[回到顶部](#top)

```py
from sklearn.linear_model import LinearRegression
import numpy as np

# 生成一些样本数据
X = np.array([[1], [2], [3], [4], [5]])  # 特征
y = np.array([2, 4, 5, 4, 5])           # 目标值

# 创建线性回归模型
model = LinearRegression()

# 训练模型
model.fit(X, y)

# 预测新数据
predictions = model.predict([[6], [7]])
print(predictions)
```




# 2. 正则化

2. **正则化**：正则化是防止过拟合的一种方法，与线性回归等算法配合使用。通过向损失函数增加惩罚项的方式对模型施加制约，有望提高模型的泛化能力。
- 二次线性回归正则化  

$$
R(\boldsymbol{w})=\sum_{i=1}^{n}\left[y_{i}-\left(w_{0}+w_{1} x_{i}+w_{2} x_{i}^{2}\right)\right]^{2}+\alpha\left(w_{1}^{2}+w_{2}^{2}\right)
$$
- Lasso回归正则化  

$$
R(\boldsymbol{w})=\sum_{i=1}^{n}\left[y_{i}-\left(w_{0}+w_{1} x_{i}+w_{2} x_{i}^{2}\right)\right]+\alpha\left(\left|w_{1}\right|+\left|w_{2}\right|\right)
$$

<p align="center">
  <img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_001.png" alt="Image Description" width="400">
</p>

```py
from sklearn.linear_model import Ridge
import numpy as np

# 生成一些样本数据
X = np.array([[1], [2], [3], [4], [5]])  # 特征
y = np.array([2, 4, 5, 4, 5])           # 目标值

# 创建Ridge回归模型
model = Ridge(alpha=1.0)  # alpha 是正则化强度

# 训练模型
model.fit(X, y)

# 预测新数据
predictions = model.predict([[6], [7]])
print(predictions)
```




# 3. 逻辑回归

3. **逻辑回归**：逻辑回归是一种用于有监督学习的分类任务的简单算法。虽然算法的名字中包含“回归”二字，但其实它是用于分类问题的算法。逻辑回归通过计算数据属于各类别的概率来进行分类。

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20231224-215916.png" alt="Image Description" width="400">
</p>

- Sigmoid 函数的图像呈S形曲线，且在z趋近正无穷大时，S(z)趋近于1；在z趋近负无穷大时，S(z)趋近于0。这使得我们可以将模型的输出理解为一个概率值，表示属于某一类别的概率。

$$
S(z) = \frac{1}{1 + e^{-z}}
$$

$$
P(Y=1) = \frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \ldots + w_nx_n)}}
$$


- 有两个输入特征的逻辑回归模型

$$
P(Y=\text{"通过"}|X_1, X_2) = \frac{1}{1 + e^{-(b_0 + b_1X_1 + b_2X_2)}}
$$

- 对数损失函数

$$
J(b_0, b_1, b_2) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log\left(P(Y=1|X_{1i}, X_{2i})\right) + (1 - y_i) \log\left(1 - P(Y=1|X_{1i}, X_{2i})\right) \right]
$$

[回到顶部](#top)

```py
from sklearn.linear_model import LogisticRegression
import numpy as np

# 生成一些样本数据
X = np.array([[0], [1], [2], [3], [4]])  # 特征
y = np.array([0, 0, 0, 1, 1])           # 目标值，二元分类标签

# 创建逻辑回归模型
model = LogisticRegression()

# 训练模型
model.fit(X, y)

# 预测新数据的分类概率
probabilities = model.predict_proba([[1.5], [2.5]])
print(probabilities)
```




# 4. 支持向量机

4. **支持向量机**：支持向量机（Support Vector Machine，SVM）是一种强大的监督学习算法，旨在进行分类和回归任务。其主要思想是在特征空间中找到一个最优的超平面，以最大化不同类别之间的间隔，从而实现高效的分类。

- 目标函数

$$
\text{最小化} \(\frac{1}{2} \|w\|^2\)
$$

- 约束条件

$$
\(y_i (w \cdot x_i + b) \geq 1\), \text{对所有的训练样本} \((x_{i1}, x_{i2}, \ldots, x_{in}, y_i)\)
$$

<p align="center">
  <img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_001.png" alt="Image Description" width="400">
  <br>
  <br>
  <img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_unbalanced_001.png" alt="Image Description" width="400">
</p>

- SVM示例：鸢尾花分类（4特征3类）

```py
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

# 加载示例数据集
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 训练SVM模型
model = SVC(kernel='linear')
model.fit(X_train, y_train)

# 预测并评估模型
y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

- `datasets.load_iris()`: 加载鸢尾花数据集。数据集包含150个样本，每个样本有4个特征（花萼长度、花萼宽度、花瓣长度和花瓣宽度），目标变量是花的种类（0, 1, 2分别表示三种鸢尾花）。
- `X = iris.data`: 提取特征数据，存储在变量X中。这里的X是一个二维数组，包含150行4列，每一行表示一个样本的4个特征。
- `y = iris.target`: 提取目标变量（标签），存储在变量y中。这里的y是一个一维数组，包含150个元素，每个元素表示对应样本的标签（花的种类）。

- `train_test_split(X, y, test_size=0.3, random_state=42)`: 将数据集按70%训练集和30%测试集的比例进行划分。`random_state=42` 确保每次运行代码时划分结果一致。
- `X_train, y_train`: 训练集的特征数据和标签。训练集用于训练模型。
- `X_test, y_test`: 测试集的特征数据和标签。测试集用于评估模型性能。

- `SVC(kernel='linear')`: 初始化支持向量分类器，使用线性核函数。
- `model.fit(X_train, y_train)`: 用训练集的数据训练SVM模型。

- `y_pred = model.predict(X_test)`: 使用训练好的模型对测试集进行预测，得到预测结果y_pred。
- `print(confusion_matrix(y_test, y_pred))`: 打印混淆矩阵，用于评估分类模型的性能。混淆矩阵显示了真实标签和预测标签的匹配情况。
- `print(classification_report(y_test, y_pred))`: 打印分类报告，包含了精确率（precision）、召回率（recall）、F1-score等评价指标。




# 5. 支持向量机（核方法）

5. 支持向量机（核方法）：支持向量机（Support Vector Machine，SVM）中的核方法是一种技术，用于处理非线性问题。在原始的SVM中，它通过在特征空间中找到一个超平面来实现线性分类或回归。然而，有时数据并不是线性可分的，这时就需要使用核方法来将数据映射到更高维的空间，使其在该空间中变得线性可分。核方法的关键思想是通过将输入数据映射到高维特征空间，使得在该空间中的线性超平面能够更好地划分不同类别。这个映射通常是通过核函数来实现的，而不是直接在原始特征空间中进行映射，从而避免了显式计算高维特征空间的复杂性。

- 线性核

$$
K(x_i, x_j) = x_i \cdot x_j
$$

- 多项式核

$$
K(x_i, x_j) = (x_i \cdot x_j + c)^d
$$


- 高斯核

$$
K(x_i, x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)
$$

<p align="center">
  <img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_svc_001.png" alt="Image Description" width="400">
</p>

[回到顶部](#top)





# 6. 朴素贝叶斯

6. `朴素贝叶斯`：朴素贝叶斯（Naive Bayes）是常用于自然语言分类问题的算法。它在垃圾邮件过滤上的应用非常有名。朴素贝叶斯是一个基于概率进行预测的算法，在实践中被用于分类问题。具体来说，就是计算数据为某个标签的概率，并将其分类为概率值最大的标签。朴素贝叶斯主要用于文本分类和垃圾邮件判定等自然语言处理中的分类问题。

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20231224-221313.png" alt="朴素贝叶斯" width="600">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20231224-221942.png" alt="朴素贝叶斯" width="600">
</p>


- 贝叶斯定理

$$
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
$$

- 展开特征的独立性假设的部分

$$
P(X|C) = P(x_1|C) \cdot P(x_2|C) \cdot \ldots \cdot P(x_n|C)
$$


- 朴素贝叶斯代码示例（也适用于数据特征是连续变量的分类场景）

```py
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix

# 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 初始化高斯朴素贝叶斯分类器
model = GaussianNB()

# 训练模型
model.fit(X_train, y_train)

# 进行预测
y_pred = model.predict(X_test)

# 打印混淆矩阵和分类报告
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
```



# 7. 随机森林

- `随机森林`：随机森林是一种集成学习方法，基于`决策树`构建而成。它通过构建多个决策树并将它们整合在一起，以提高整体模型的性能和鲁棒性。
- 随机森林在构建每棵决策树时引入了两个主要的随机性源：首先，在选择每个节点的划分特征时，随机选择一部分特征进行考虑；其次，在训练每棵决策树时，采用自助采样方法，有放回地从原始数据集中抽取样本。
- 最终，随机森林通过投票机制（对于分类问题）或取平均值（对于回归问题）的方式，综合多个决策树的意见，产生最终的预测结果。这种集成方法使得随机森林对于噪声数据具有鲁棒性，同时能够在处理大量数据和高维特征的情况下表现出色。

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20231225-092704.png" alt="Image Description" width="600">
</p>


💎 **决策树示例代码**

下面这段代码加载了 Iris 数据集，训练了一个决策树分类器，并使用 Matplotlib 和 Graphviz 可视化了决策树。


```py
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz
import graphviz

# 加载数据集
iris = load_iris()
X, y = iris.data, iris.target

# 创建并训练决策树分类器
clf = DecisionTreeClassifier(random_state=0)
clf.fit(X, y)

# 使用 matplotlib 绘制决策树
plt.figure(figsize=(20,10))
plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)
plt.title("Decision Tree of the Iris Dataset")
plt.show()

# 使用 Graphviz 绘制决策树
dot_data = export_graphviz(clf, out_file=None, 
                           feature_names=iris.feature_names,
                           class_names=iris.target_names,
                           filled=True, rounded=True,
                           special_characters=True)
graph = graphviz.Source(dot_data)
graph.render("iris_decision_tree")  # 保存到文件
graph.view()  # 显示图形
```

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240614-154206.png" alt="Image Description" width="850">
</p>


[回到顶部](#top)





# 8. 神经网络

- 神经网络：神经网络是一种计算模型，灵感来源于生物学中神经系统的结构和功能。它由多个基本单元组成，这些单元被称为神经元，通过连接权重相互链接形成一个网络。

- 神经网络通常分为输入层、隐藏层和输出层。每个神经元接收输入，通过与输入相乘的权重进行加权，然后通过激活函数产生输出，这个输出作为下一层神经元的输入。通过多层次的连接和非线性激活函数，神经网络能够学习和表示复杂的模式和特征，使其成为机器学习和深度学习中的关键工具。

- 神经网络通过反向传播算法进行训练，不断调整连接权重，使网络的输出更接近期望的结果，从而能够执行诸如分类、回归、图像识别等多种任务。深度学习中的深度神经网络（DNN）具有多个隐藏层，使其能够处理更复杂的任务和数据。

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20231229-180831.png" alt="神经网络" width="600">
</p>


💎 **多层感知机（MLP）**

- `多层感知机（MLP）`是一种特定类型的前馈神经网络，由多个层次组成，包括至少一个隐藏层。MLP 专注于通过层与层之间的全连接实现模式识别和分类任务。

- `神经网络`是一个更广泛的概念，包括 MLP 以及其他如`卷积神经网络（CNN）`和`循环神经网络（RNN）`等结构，它们可以处理更复杂的数据类型如图像和序列数据。简而言之，所有的 MLP 都是神经网络，但不是所有的神经网络都是 MLP；MLP 是神经网络的一种简化且特化的形式，主要用于表达层间连接和数据流的简单和直接的架构。

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240613-105550.png" alt="Image Description" width="450">
</p>

- 手写数字数据集的训练和预测（使用 sklearn 的 MLPClassifier）

```py
from sklearn.datasets import load_digits
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 读取数据
data = load_digits()
X = data.images.reshape(len(data.images), -1)
y = data.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 初始化和训练模型
model = MLPClassifier(hidden_layer_sizes=(16,), max_iter=1000, learning_rate_init=0.005, random_state=42)
model.fit(X_train, y_train)

# 进行预测并评估
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
```

- 数据加载：使用 `load_digits()` 函数加载内置的手写数字数据集，它包含了1797个样本，每个样本是一个8x8的图像。
- 数据预处理：将每个8x8的图像数据拉平成64个特征的一维数组，以便能够作为MLP的输入。
- 数据划分：使用 `train_test_split()` 将数据集随机分割为训练集和测试集，其中测试集占总数据的30%。
- 模型定义：定义一个 `MLPClassifier`，设置一个包含16个神经元的隐藏层。这里 `random_state` 被设置为42，用于确保每次运行代码时，划分的数据和初始化的权重保持一致，以便复现结果。
- 模型训练：调用 fit() 方法训练模型，使用训练集的特征和标签。
- 预测和评估：使用 predict() 方法对测试集进行预测，然后通过 `accuracy_score()` 计算模型的准确率并输出。


💎 **总结**

在上述代码中，使用的多层感知机（MLP）模型结构由输入层、一个隐藏层以及输出层组成。具体如下：

1. 输入层：输入层的大小由数据集的特征数量决定。在这个示例中，使用的是手写数字数据集（digits），每个图像被展平成一维数组，每个图像的大小为 8x8 像素，因此输入层有 64 个神经元（8x8=64），对应于每个图像的每个像素。

2. 隐藏层：在这个示例中，隐藏层配置为只有一层，其中包含 16 个神经元。这个数值是可以调整的，更多的神经元或隐藏层可能会增强模型的学习能力，但同时也可能导致过拟合，特别是在小型数据集上。隐藏层中的神经元使用激活函数来引入非线性，使得网络能够学习更复杂的数据模式。MLPClassifier 默认使用 'relu'（线性整流函数）作为激活函数。

3. 输出层：输出层的神经元数量由目标类别的数量决定。在这个例子中，digits 数据集有 10 个不同的类别（数字0到9），因此输出层有 10 个神经元。输出层通常使用 softmax 激活函数，这在多分类问题中非常有用，因为 softmax 函数可以将输出解释为概率分布。


- `scikit-learn` 参考资料：https://scikit-learn.org/stable/modules/neural_networks_supervised.html




# 9. K最近邻

- `K最近邻（K-Nearest Neighbors，KNN）`是一种简单而直观的监督学习算法，用于分类和回归任务。其基本思想是基于特征空间中数据点的邻近性，即认为相似的样本在特征空间中也是相邻的。

- 在KNN中，每个数据点都被看作是特征空间中的一个点，分类或预测新样本时，**算法会找到离该点最近的K个已知类别的样本，然后通过这K个邻居中最常见的类别（对于分类任务）或平均值（对于回归任务）来决定新样本的类别或预测值。**

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20231225-092512.png" alt="Image Description" width="600">
</p>


💎 **示例代码：鸢尾花分类**

鸢尾花数据集包含了150个样本，每个样本有4个特征和对应的类别标签（三种鸢尾花之一）

```py
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

# 加载鸢尾花数据集
data = load_iris()
X = data.data
y = data.target

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建 KNN 模型，k 设为 3
knn = KNeighborsClassifier(n_neighbors=3)

# 在训练数据上训练模型
knn.fit(X_train, y_train)

# 测试模型的准确度
accuracy = knn.score(X_test, y_test)
print("准确度：", accuracy)
```

💎 **可视化特征空间**

由于鸢尾花数据集有四个特征，为简化可视化，我们只选用两个特征（例如，花瓣长度和花瓣宽度）。

```py
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
import numpy as np

# 加载数据集
iris = load_iris()
X = iris.data[:, 2:4]  # 选择花瓣长度和宽度
y = iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建并训练模型
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# 创建颜色映射
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

# 绘制决策边界
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, .02),
                     np.arange(y_min, y_max, .02))
Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

# 绘制训练点
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold, edgecolor='k', s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("3-Class classification (k = 3, weights = 'uniform')")
plt.show()
```

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240614-144523.png" alt="Image Description" width="450">
</p>


[回到顶部](#top)







# 参考资料

1. https://github.com/Yiwei666/11_kaggle_ML/wiki/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95#2-%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5








