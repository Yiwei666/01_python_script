# 深度学习

# 1. 基本概念

### 1. 条件分布的最大似然估计

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240704-110530.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240704-110743.png" alt="Image Description" width="700">
</p>

### 2. 交叉熵和KL散度

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-142123.png" alt="Image Description" width="700">
</p>

### 3. 损失函数

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-163959.png" alt="Image Description" width="700">
</p>

### 4. 激活函数（输出单元）

- 典型输出单元

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-163525.png" alt="Image Description" width="700">
</p>

- 不同输出单元的损失函数

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240704-142801.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240704-143021.png" alt="Image Description" width="700">
</p>

- 图像

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-113622.png" alt="Image Description" width="700">
</p>

- 绘图代码

```py
import numpy as np
import matplotlib.pyplot as plt

# 定义Sigmoid函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 定义Tanh函数
def tanh(x):
    return np.tanh(x)

# 定义ReLU函数
def relu(x):
    return np.maximum(0, x)

# 定义Softmax函数
def softmax(x):
    e_x = np.exp(x - np.max(x))  # 减去最大值以提高数值稳定性
    return e_x / e_x.sum(axis=0)

# 生成x值
x = np.linspace(-10, 10, 100)

# 创建一个图形和四个子图（2x2布局）
fig, axs = plt.subplots(2, 2, figsize=(12, 10))

# 绘制Sigmoid函数
axs[0, 0].plot(x, sigmoid(x), label='Sigmoid', color='blue')
axs[0, 0].set_title('Sigmoid Activation Function')
axs[0, 0].grid(True)
axs[0, 0].set_xlabel('X')
axs[0, 0].set_ylabel('Y')

# 绘制Tanh函数
axs[0, 1].plot(x, tanh(x), label='Tanh', color='red')
axs[0, 1].set_title('Tanh Activation Function')
axs[0, 1].grid(True)
axs[0, 1].set_xlabel('X')
axs[0, 1].set_ylabel('Y')

# 绘制ReLU函数
axs[1, 0].plot(x, relu(x), label='ReLU', color='green')
axs[1, 0].set_title('ReLU Activation Function')
axs[1, 0].grid(True)
axs[1, 0].set_xlabel('X')
axs[1, 0].set_ylabel('Y')

# 绘制Softmax函数
# 注意：Softmax通常用于多类分类，这里为了示范，我们只是展示单变量的Softmax
axs[1, 1].plot(x, softmax(x), label='Softmax', color='purple')
axs[1, 1].set_title('Softmax Activation Function')
axs[1, 1].grid(True)
axs[1, 1].set_xlabel('X')
axs[1, 1].set_ylabel('Y')

# 为每个子图添加图例
for ax in np.ravel(axs):  # 使用 np.ravel 处理索引
    ax.legend()

# 自动调整子图间距
plt.tight_layout()

# 显示图形
plt.show()
```

### 5. 梯度下降和学习率

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-143101.png" alt="Image Description" width="700">
</p>

### 6. 随机梯度下降和小批量梯度下降

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-143727.png" alt="Image Description" width="700">
</p>


### 7. 误差反向传播法

1. 基本概念

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172253.png" alt="Image Description" width="700">
</p>

2. 理论推导

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172636.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172851.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-173024.png" alt="Image Description" width="700">
</p>

3. 实例化

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-174356.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-174749.png" alt="Image Description" width="700">
</p>


### 8. 万能逼近定理

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240704-161246.png" alt="Image Description" width="700">
</p>


# 2. 性能度量

### 1. 概述

1. 性能度量的定义：

- 性能度量是指用于衡量模型在预测、分类或其他任务中的表现的各种数值指标。这些指标旨在描述模型的正确性和错误率，以及在错误发生时的具体情况。


### 2. 概率基础

- 性能度量通常基于概率理论，描述模型在不同情况下的预测概率。文档介绍了简单概率、条件概率和联合概率等基本概念，并通过投掷飞镖的比喻来解释这些概率的计算和意义。

- 条件概率和联合概率示意图

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240710-113322.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240709-214817.png" alt="Image Description" width="700">
</p>




# 3. 全连接神经网络（FCN）

### 1. 神经元和向前传播

1. 神经元示意图

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240704-151742.png" alt="Image Description" width="450">
</p>

2. 深度神经网络示意图

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240704-151832.png" alt="Image Description" width="600">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240709-173427.png" alt="Image Description" width="800">
</p>


3. 向前传播的矩阵和向量一般表示

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240704-152027.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240704-152106.png" alt="Image Description" width="700">
</p>



### 2. 反向传播演示

为了演示梯度下降在神经网络中的应用，我们可以通过一个简单的神经网络示例，其中包含一个输入层、一个隐藏层（只有一个神经元）和一个输出层（同样只有一个神经元），来预测一些线性数据。

假设我们有一个简单的线性关系 `y=2x+1`，我们的目标是训练一个神经网络来学习这个关系。

```py
import numpy as np
import matplotlib.pyplot as plt

# 使用NumPy生成线性数据，并添加一些高斯噪声
np.random.seed(42)  # 设置随机种子以确保结果可重复
x = np.linspace(-1, 1, 100)  # 生成100个等间隔的x值，范围从-1到1
y = 2 * x + 1 + np.random.randn(*x.shape) * 0.4  # 真实的y值基于线性关系2x + 1，并添加噪声

# 初始化神经网络参数：权重和偏置
w1 = np.random.randn()  # 隐藏层权重
b1 = np.random.randn()  # 隐藏层偏置
w2 = np.random.randn()  # 输出层权重
b2 = np.random.randn()  # 输出层偏置

# 设置学习率
learning_rate = 0.01

# 定义激活函数及其导数：这里使用线性激活函数
def activation(z):
    return z  # 线性激活

def activation_prime(z):
    return 1  # 线性激活函数的导数

# 训练过程
num_epochs = 100  # 设置迭代次数
for epoch in range(num_epochs):
    for i in range(len(x)):
        # 前向传播
        z1 = x[i] * w1 + b1  # 计算隐藏层的输入
        a1 = activation(z1)  # 计算隐藏层的输出
        z2 = a1 * w2 + b2  # 计算输出层的输入
        a2 = activation(z2)  # 计算输出层的输出

        # 计算损失（均方误差）
        loss = (a2 - y[i]) ** 2

        # 反向传播：计算损失函数关于各个参数的导数
        dloss_da2 = 2 * (a2 - y[i])
        da2_dz2 = activation_prime(z2)
        dz2_dw2 = a1
        dz2_db2 = 1
        dz2_da1 = w2
        da1_dz1 = activation_prime(z1)
        dz1_dw1 = x[i]
        dz1_db1 = 1

        # 计算梯度
        dloss_dw2 = dloss_da2 * da2_dz2 * dz2_dw2
        dloss_db2 = dloss_da2 * da2_dz2 * dz2_db2
        dloss_dw1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_dw1
        dloss_db1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_db1

        # 更新权重和偏置
        w2 -= learning_rate * dloss_dw2
        b2 -= learning_rate * dloss_db2
        w1 -= learning_rate * dloss_dw1
        b1 -= learning_rate * dloss_db1

# 绘制训练结果
predicted = activation(activation(x * w1 + b1) * w2 + b2)
plt.scatter(x, y, color='red', label='Data Points')  # 原始数据点
plt.plot(x, predicted, label='Fitted Line')  # 拟合曲线
plt.legend()
plt.show()
```

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-151840.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-151924.png" alt="Image Description" width="700">
</p>


### 3. 训练过程动态视图

💎 图像

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-153603.png" alt="Image Description" width="800">
</p>

1. 损失历史（Loss History）
   - 观察到的现象：损失值在开始时迅速下降，这表明模型在最初迭代中学习了大量的信息。随后，损失值趋于稳定，呈现出一种周期性的小波动。
   - 可能的解释：初始快速下降可能由于权重和偏置从随机值向更优解调整。稳定后的小波动可能是因为学习率相对较小或模型已接近最优解但还在微调。

2. 权重历史（Weight w1 and w2 History）
   - 权重 w1 和 w2：两个权重都显示出快速的初始变化，然后逐渐平稳。w1 和 w2 的变化趋势表明，模型参数在经过初期调整后基本稳定。
   - 权重调整：快速的初始变化可能是模型在试图找到适应数据的最佳方式。之后变化平缓可能意味着模型已经找到了合适的参数设置。

3. 偏置历史（Bias b1 and b2 History）
   - 偏置 b1 和 b2：偏置的变化趋势与权重类似，初始有较大的调整，随后变化幅度减小。b1 在调整后趋于稳定，而 b2 在初始跳跃后也趋于平稳。
   - 影响解释：偏置的调整帮助模型在特征空间中正确定位决策边界。稳定后的偏置表明模型在数据上的偏差已经较小。

4. 模型拟合（Model Fit）
   - 拟合效果：从模型拟合图看，拟合线（蓝色）与数据点（红色）非常接近，说明模型有效地学习了数据中的线性关系。
   - 效果评估：模型能够很好地预测或近似真实数据分布，这表明所选的网络架构和训练策略是有效的。



💎 绘图代码

```py
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

# 使用NumPy生成线性数据，并添加一些高斯噪声
np.random.seed(42)  # 设置随机种子以确保结果可重复
x = np.linspace(-1, 1, 100)  # 生成100个等间隔的x值，范围从-1到1
y = 2 * x + 1 + np.random.randn(*x.shape) * 0.4  # 真实的y值基于线性关系2x + 1，并添加噪声

# 初始化神经网络参数：权重和偏置
w1 = np.random.randn()  # 隐藏层权重
b1 = np.random.randn()  # 隐藏层偏置
w2 = np.random.randn()  # 输出层权重
b2 = np.random.randn()  # 输出层偏置

# 设置学习率
learning_rate = 0.01

# 定义激活函数及其导数：这里使用线性激活函数
def activation(z):
    return z  # 线性激活

def activation_prime(z):
    return 1  # 线性激活函数的导数

# 初始化记录损失和参数变化的列表
loss_history = []
w1_history, b1_history, w2_history, b2_history = [], [], [], []

# 训练过程
num_epochs = 100  # 设置迭代次数
for epoch in tqdm(range(num_epochs), desc="Training Progress"):
    for i in range(len(x)):
        # 前向传播
        z1 = x[i] * w1 + b1  # 计算隐藏层的输入
        a1 = activation(z1)  # 计算隐藏层的输出
        z2 = a1 * w2 + b2  # 计算输出层的输入
        a2 = activation(z2)  # 计算输出层的输出

        # 计算损失（均方误差）
        loss = (a2 - y[i])**2
        loss_history.append(loss)  # 记录每次的损失值

        # 反向传播：计算损失函数关于各个参数的导数
        dloss_da2 = 2 * (a2 - y[i])
        da2_dz2 = activation_prime(z2)
        dz2_dw2 = a1
        dz2_db2 = 1
        dz2_da1 = w2
        da1_dz1 = activation_prime(z1)
        dz1_dw1 = x[i]
        dz1_db1 = 1

        # 计算梯度
        dloss_dw2 = dloss_da2 * da2_dz2 * dz2_dw2
        dloss_db2 = dloss_da2 * da2_dz2 * dz2_db2
        dloss_dw1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_dw1
        dloss_db1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_db1

        # 更新权重和偏置
        w2 -= learning_rate * dloss_dw2
        b2 -= learning_rate * dloss_db2
        w1 -= learning_rate * dloss_dw1
        b1 -= learning_rate * dloss_db1

        # 保存权重和偏置历史
        w1_history.append(w1)
        b1_history.append(b1)
        w2_history.append(w2)
        b2_history.append(b2)

# 绘制损失历史
plt.figure(figsize=(14, 7))
plt.subplot(2, 3, 1)
plt.plot(loss_history, label='Loss over time')
plt.title('Loss History')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.legend()

# 绘制权重和偏置变化
plt.subplot(2, 3, 2)
plt.plot(w1_history, label='w1')
plt.title('Weight w1 History')
plt.legend()

plt.subplot(2, 3, 3)
plt.plot(b1_history, label='b1')
plt.title('Bias b1 History')
plt.legend()

plt.subplot(2, 3, 4)
plt.plot(w2_history, label='w2')
plt.title('Weight w2 History')
plt.legend()

plt.subplot(2, 3, 5)
plt.plot(b2_history, label='b2')
plt.title('Bias b2 History')
plt.legend()

# 绘制模型拟合结果
plt.subplot(2, 3, 6)
plt.scatter(x, y, color='red', label='Data Points')  # 原始数据点
predicted = activation(activation(x * w1 + b1) * w2 + b2)
plt.plot(x, predicted, label='Fitted Line', color='blue')  # 拟合曲线
plt.title('Model Fit')
plt.legend()

plt.tight_layout()
plt.show()
```

### 4. 全连接神经网络缺点

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-160849.png" alt="Image Description" width="700">
</p>




# 4. Iris 多层感知机（MLP）

### 1. 概述

1. 定义：多层感知机（Multilayer Perceptron, MLP）是一种前馈神经网络，通常由输入层、一个或多个隐藏层和输出层组成。每一层的神经元与上一层的所有神经元相连接。

2. 特点：

- 全连接层：MLP 中每个神经元与前一层的所有神经元相连接，这称为全连接层（Fully Connected Layer）。
- 非线性激活函数：常用的激活函数包括 ReLU（Rectified Linear Unit）、Sigmoid 和 Tanh。
- 适用于结构化数据：MLP 通常用于处理结构化数据，如表格数据。
- 简单的架构：架构较为简单，适用于较小的数据集和任务。



### 2. 鸢尾花 MLP

1. 决策边界

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240627-153841.png" alt="Image Description" width="600">
</p>


2. 源代码

```py
# 导入必要的库
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.decomposition import PCA

# 加载数据集
iris = load_iris()
X = iris.data
y = iris.target

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 使用PCA降维到2维
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 将数据集拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# 构建MLP分类器
mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42)
mlp.fit(X_train, y_train)

# 决策边界的可视化
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))

# 使用分类器对网格点进行预测
Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# 绘制决策边界
plt.figure(figsize=(10, 5))
plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='k', s=20)
plt.title('Decision Boundary after PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
```

📌 代码解释

- 数据标准化：我们使用StandardScaler对数据进行标准化处理。
- PCA降维：使用PCA将数据从4维降到2维。
- 数据集拆分：将降维后的数据集拆分为训练集和测试集。
- 训练MLP分类器：构建并训练一个具有`两个隐藏层`的MLP分类器。
- 生成网格并预测类别：在二维特征空间中生成一个网格点集合，并使用训练好的分类器对这些网格点进行预测。
- 绘制决策边界：使用contourf函数绘制决策边界，并使用scatter函数绘制实际数据点。



# 5. 卷积神经网络（CNN）

###  1. 卷积核提取特征

- 滤波器和特征图

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240827-164042.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-161554.png" alt="Image Description" width="700">
</p>



### 2. 卷积层和池化层

- 卷积层和池化层

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240827-163143.png" alt="Image Description" width="700">
</p>


<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-162208.png" alt="Image Description" width="700">
</p>

💎 **卷积层的基本构成**

1. 卷积操作：

   - 卷积层通过执行卷积操作来处理输入数据（通常是图像）。这种操作包括将一组小型的、可学习的滤波器（或称为卷积核）应用于输入数据。
   - 每个滤波器都在输入图像的全局或部分区域上滑动，对应的区域进行元素乘积后求和，生成特征图的一个像素点。这个过程在整个输入图像上重复进行，以形成完整的特征图。

2. 滤波器和特征图：

   - 滤波器负责从输入图像中提取特定类型的特征，如边缘、角点、纹理等。每个滤波器都对应于输出的一个特征图，显示了输入图像在该滤波器表示的特征上的响应强度和位置。
   - 在实际应用中，卷积层可以包含多个滤波器，每个滤波器产生一个独立的特征图，这些特征图堆叠在一起形成输出数据。

💎 **卷积层的关键参数**

3. 步长（Stride）：

   - 步长定义了滤波器在输入图像上滑动时每次移动的像素数。较大的步长会减小输出特征图的尺寸，而较小的步长能生成更详细的特征图。

4. 填充（Padding）：

   - 填充是指在输入图像的边界周围填充一定数量的像素（通常是0），以允许滤波器覆盖到图像的边缘部分。这有助于控制输出特征图的空间尺寸，并保持输入和输出尺寸的一致性。
     
💎 **卷积层的优点和功能**

5. 参数共享：

   - 在卷积层中，每个滤波器的参数（即卷积核的权重）在整个输入图像上是共享的。这种参数共享显著减少了模型的参数数量，降低了过拟合的风险，并提高了模型的泛化能力。

6. 局部连接：

   - 卷积层采用局部连接的策略，即每个神经元仅与输入数据的一个局部区域相连接。这利用了图像数据的局部空间关联性，使模型能够捕捉到局部特征信息，并减少了计算复杂度。




### 3. 池化层


### 4. LeNet

### 5. AlexNet



# 6. 卷积神经网络分类MNIST手写数字

### 1. 代码


📌 **说明**

下面代码实现了一个简单的卷积神经网络（CNN）来识别MNIST手写数字数据集。具体步骤包括：

1. 加载并预处理MNIST数据集，将图像数据调整为适合模型输入的形状并归一化。
2. 构建一个卷积神经网络模型，包括卷积层、池化层、全连接层，以及最终的分类层。
3. 编译模型，使用Adam优化器和交叉熵损失函数，并定义准确率作为评估指标。
4. 训练模型，使用训练集对模型进行训练，并在训练过程中进行验证。
5. 评估模型，使用测试集评估模型的性能，并打印测试准确率。
6. 生成预测结果，并可视化前10张测试图像的预测结果，通过颜色区分预测是否正确。

```py
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np

# 加载MNIST数据集
(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()

# 数据预处理：调整形状并归一化到0-1之间
train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255

# 构建卷积神经网络模型
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# 显示模型结构
model.summary()

# 编译模型
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 训练模型
model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.1)

# 评估模型
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test accuracy: {test_acc}")

# 生成预测
predictions = model.predict(test_images)

# 可视化预测结果
def plot_image(i, predictions_array, true_label, img):
    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img.reshape(28, 28), cmap=plt.cm.binary)

    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
        color = 'blue'
    else:
        color = 'red'

    plt.xlabel(f"{predicted_label} ({true_label})", color=color)

for i in range(10):
    plt.figure(figsize=(6,3))
    plt.subplot(1,2,1)
    plot_image(i, predictions, test_labels, test_images)
    plt.show()
```


### 2. 代码注释

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240827-152256.png" alt="Image Description" width="700">
</p>





# 7. 自编码器



# 参考文献

1. "The Hundred-Page Machine Learning Book" by Andriy Burkov
2. "Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville


