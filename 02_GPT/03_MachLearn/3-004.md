# æ·±åº¦å­¦ä¹ 

# 1. åŸºæœ¬æ¦‚å¿µ

### 1. äº¤å‰ç†µå’ŒKLæ•£åº¦

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-142123.png" alt="Image Description" width="700">
</p>

### 2. æŸå¤±å‡½æ•°

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-163959.png" alt="Image Description" width="700">
</p>

### 3. æ¿€æ´»å‡½æ•°

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-163525.png" alt="Image Description" width="700">
</p>

- å›¾åƒ

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-113622.png" alt="Image Description" width="700">
</p>

- ç»˜å›¾ä»£ç 

```py
import numpy as np
import matplotlib.pyplot as plt

# å®šä¹‰Sigmoidå‡½æ•°
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# å®šä¹‰Tanhå‡½æ•°
def tanh(x):
    return np.tanh(x)

# å®šä¹‰ReLUå‡½æ•°
def relu(x):
    return np.maximum(0, x)

# å®šä¹‰Softmaxå‡½æ•°
def softmax(x):
    e_x = np.exp(x - np.max(x))  # å‡å»æœ€å¤§å€¼ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
    return e_x / e_x.sum(axis=0)

# ç”Ÿæˆxå€¼
x = np.linspace(-10, 10, 100)

# åˆ›å»ºä¸€ä¸ªå›¾å½¢å’Œå››ä¸ªå­å›¾ï¼ˆ2x2å¸ƒå±€ï¼‰
fig, axs = plt.subplots(2, 2, figsize=(12, 10))

# ç»˜åˆ¶Sigmoidå‡½æ•°
axs[0, 0].plot(x, sigmoid(x), label='Sigmoid', color='blue')
axs[0, 0].set_title('Sigmoid Activation Function')
axs[0, 0].grid(True)
axs[0, 0].set_xlabel('X')
axs[0, 0].set_ylabel('Y')

# ç»˜åˆ¶Tanhå‡½æ•°
axs[0, 1].plot(x, tanh(x), label='Tanh', color='red')
axs[0, 1].set_title('Tanh Activation Function')
axs[0, 1].grid(True)
axs[0, 1].set_xlabel('X')
axs[0, 1].set_ylabel('Y')

# ç»˜åˆ¶ReLUå‡½æ•°
axs[1, 0].plot(x, relu(x), label='ReLU', color='green')
axs[1, 0].set_title('ReLU Activation Function')
axs[1, 0].grid(True)
axs[1, 0].set_xlabel('X')
axs[1, 0].set_ylabel('Y')

# ç»˜åˆ¶Softmaxå‡½æ•°
# æ³¨æ„ï¼šSoftmaxé€šå¸¸ç”¨äºå¤šç±»åˆ†ç±»ï¼Œè¿™é‡Œä¸ºäº†ç¤ºèŒƒï¼Œæˆ‘ä»¬åªæ˜¯å±•ç¤ºå•å˜é‡çš„Softmax
axs[1, 1].plot(x, softmax(x), label='Softmax', color='purple')
axs[1, 1].set_title('Softmax Activation Function')
axs[1, 1].grid(True)
axs[1, 1].set_xlabel('X')
axs[1, 1].set_ylabel('Y')

# ä¸ºæ¯ä¸ªå­å›¾æ·»åŠ å›¾ä¾‹
for ax in np.ravel(axs):  # ä½¿ç”¨ np.ravel å¤„ç†ç´¢å¼•
    ax.legend()

# è‡ªåŠ¨è°ƒæ•´å­å›¾é—´è·
plt.tight_layout()

# æ˜¾ç¤ºå›¾å½¢
plt.show()
```

### 4. æ¢¯åº¦ä¸‹é™å’Œå­¦ä¹ ç‡

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-143101.png" alt="Image Description" width="700">
</p>

### 5. éšæœºæ¢¯åº¦ä¸‹é™å’Œå°æ‰¹é‡æ¢¯åº¦ä¸‹é™

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-143727.png" alt="Image Description" width="700">
</p>


### 6. è¯¯å·®åå‘ä¼ æ’­æ³•

1. åŸºæœ¬æ¦‚å¿µ

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172253.png" alt="Image Description" width="700">
</p>

2. ç†è®ºæ¨å¯¼

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172636.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172851.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-173024.png" alt="Image Description" width="700">
</p>

3. å®ä¾‹åŒ–

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-174356.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-174749.png" alt="Image Description" width="700">
</p>





# 2. ç¥ç»ç½‘ç»œ

### 1. åå‘ä¼ æ’­æ¼”ç¤º

ä¸ºäº†æ¼”ç¤ºæ¢¯åº¦ä¸‹é™åœ¨ç¥ç»ç½‘ç»œä¸­çš„åº”ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œç¤ºä¾‹ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªè¾“å…¥å±‚ã€ä¸€ä¸ªéšè—å±‚ï¼ˆåªæœ‰ä¸€ä¸ªç¥ç»å…ƒï¼‰å’Œä¸€ä¸ªè¾“å‡ºå±‚ï¼ˆåŒæ ·åªæœ‰ä¸€ä¸ªç¥ç»å…ƒï¼‰ï¼Œæ¥é¢„æµ‹ä¸€äº›çº¿æ€§æ•°æ®ã€‚

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„çº¿æ€§å…³ç³» `y=2x+1`ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥å­¦ä¹ è¿™ä¸ªå…³ç³»ã€‚

```py
import numpy as np
import matplotlib.pyplot as plt

# ä½¿ç”¨NumPyç”Ÿæˆçº¿æ€§æ•°æ®ï¼Œå¹¶æ·»åŠ ä¸€äº›é«˜æ–¯å™ªå£°
np.random.seed(42)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤
x = np.linspace(-1, 1, 100)  # ç”Ÿæˆ100ä¸ªç­‰é—´éš”çš„xå€¼ï¼ŒèŒƒå›´ä»-1åˆ°1
y = 2 * x + 1 + np.random.randn(*x.shape) * 0.4  # çœŸå®çš„yå€¼åŸºäºçº¿æ€§å…³ç³»2x + 1ï¼Œå¹¶æ·»åŠ å™ªå£°

# åˆå§‹åŒ–ç¥ç»ç½‘ç»œå‚æ•°ï¼šæƒé‡å’Œåç½®
w1 = np.random.randn()  # éšè—å±‚æƒé‡
b1 = np.random.randn()  # éšè—å±‚åç½®
w2 = np.random.randn()  # è¾“å‡ºå±‚æƒé‡
b2 = np.random.randn()  # è¾“å‡ºå±‚åç½®

# è®¾ç½®å­¦ä¹ ç‡
learning_rate = 0.01

# å®šä¹‰æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°ï¼šè¿™é‡Œä½¿ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°
def activation(z):
    return z  # çº¿æ€§æ¿€æ´»

def activation_prime(z):
    return 1  # çº¿æ€§æ¿€æ´»å‡½æ•°çš„å¯¼æ•°

# è®­ç»ƒè¿‡ç¨‹
num_epochs = 100  # è®¾ç½®è¿­ä»£æ¬¡æ•°
for epoch in range(num_epochs):
    for i in range(len(x)):
        # å‰å‘ä¼ æ’­
        z1 = x[i] * w1 + b1  # è®¡ç®—éšè—å±‚çš„è¾“å…¥
        a1 = activation(z1)  # è®¡ç®—éšè—å±‚çš„è¾“å‡º
        z2 = a1 * w2 + b2  # è®¡ç®—è¾“å‡ºå±‚çš„è¾“å…¥
        a2 = activation(z2)  # è®¡ç®—è¾“å‡ºå±‚çš„è¾“å‡º

        # è®¡ç®—æŸå¤±ï¼ˆå‡æ–¹è¯¯å·®ï¼‰
        loss = (a2 - y[i]) ** 2

        # åå‘ä¼ æ’­ï¼šè®¡ç®—æŸå¤±å‡½æ•°å…³äºå„ä¸ªå‚æ•°çš„å¯¼æ•°
        dloss_da2 = 2 * (a2 - y[i])
        da2_dz2 = activation_prime(z2)
        dz2_dw2 = a1
        dz2_db2 = 1
        dz2_da1 = w2
        da1_dz1 = activation_prime(z1)
        dz1_dw1 = x[i]
        dz1_db1 = 1

        # è®¡ç®—æ¢¯åº¦
        dloss_dw2 = dloss_da2 * da2_dz2 * dz2_dw2
        dloss_db2 = dloss_da2 * da2_dz2 * dz2_db2
        dloss_dw1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_dw1
        dloss_db1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_db1

        # æ›´æ–°æƒé‡å’Œåç½®
        w2 -= learning_rate * dloss_dw2
        b2 -= learning_rate * dloss_db2
        w1 -= learning_rate * dloss_dw1
        b1 -= learning_rate * dloss_db1

# ç»˜åˆ¶è®­ç»ƒç»“æœ
predicted = activation(activation(x * w1 + b1) * w2 + b2)
plt.scatter(x, y, color='red', label='Data Points')  # åŸå§‹æ•°æ®ç‚¹
plt.plot(x, predicted, label='Fitted Line')  # æ‹Ÿåˆæ›²çº¿
plt.legend()
plt.show()
```

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-151840.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-151924.png" alt="Image Description" width="700">
</p>


### 2. è®­ç»ƒè¿‡ç¨‹åŠ¨æ€è§†å›¾

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-153603.png" alt="Image Description" width="700">
</p>

- ç»˜å›¾ä»£ç 

```py
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

# ä½¿ç”¨NumPyç”Ÿæˆçº¿æ€§æ•°æ®ï¼Œå¹¶æ·»åŠ ä¸€äº›é«˜æ–¯å™ªå£°
np.random.seed(42)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤
x = np.linspace(-1, 1, 100)  # ç”Ÿæˆ100ä¸ªç­‰é—´éš”çš„xå€¼ï¼ŒèŒƒå›´ä»-1åˆ°1
y = 2 * x + 1 + np.random.randn(*x.shape) * 0.4  # çœŸå®çš„yå€¼åŸºäºçº¿æ€§å…³ç³»2x + 1ï¼Œå¹¶æ·»åŠ å™ªå£°

# åˆå§‹åŒ–ç¥ç»ç½‘ç»œå‚æ•°ï¼šæƒé‡å’Œåç½®
w1 = np.random.randn()  # éšè—å±‚æƒé‡
b1 = np.random.randn()  # éšè—å±‚åç½®
w2 = np.random.randn()  # è¾“å‡ºå±‚æƒé‡
b2 = np.random.randn()  # è¾“å‡ºå±‚åç½®

# è®¾ç½®å­¦ä¹ ç‡
learning_rate = 0.01

# å®šä¹‰æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°ï¼šè¿™é‡Œä½¿ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°
def activation(z):
    return z  # çº¿æ€§æ¿€æ´»

def activation_prime(z):
    return 1  # çº¿æ€§æ¿€æ´»å‡½æ•°çš„å¯¼æ•°

# åˆå§‹åŒ–è®°å½•æŸå¤±å’Œå‚æ•°å˜åŒ–çš„åˆ—è¡¨
loss_history = []
w1_history, b1_history, w2_history, b2_history = [], [], [], []

# è®­ç»ƒè¿‡ç¨‹
num_epochs = 100  # è®¾ç½®è¿­ä»£æ¬¡æ•°
for epoch in tqdm(range(num_epochs), desc="Training Progress"):
    for i in range(len(x)):
        # å‰å‘ä¼ æ’­
        z1 = x[i] * w1 + b1  # è®¡ç®—éšè—å±‚çš„è¾“å…¥
        a1 = activation(z1)  # è®¡ç®—éšè—å±‚çš„è¾“å‡º
        z2 = a1 * w2 + b2  # è®¡ç®—è¾“å‡ºå±‚çš„è¾“å…¥
        a2 = activation(z2)  # è®¡ç®—è¾“å‡ºå±‚çš„è¾“å‡º

        # è®¡ç®—æŸå¤±ï¼ˆå‡æ–¹è¯¯å·®ï¼‰
        loss = (a2 - y[i])**2
        loss_history.append(loss)  # è®°å½•æ¯æ¬¡çš„æŸå¤±å€¼

        # åå‘ä¼ æ’­ï¼šè®¡ç®—æŸå¤±å‡½æ•°å…³äºå„ä¸ªå‚æ•°çš„å¯¼æ•°
        dloss_da2 = 2 * (a2 - y[i])
        da2_dz2 = activation_prime(z2)
        dz2_dw2 = a1
        dz2_db2 = 1
        dz2_da1 = w2
        da1_dz1 = activation_prime(z1)
        dz1_dw1 = x[i]
        dz1_db1 = 1

        # è®¡ç®—æ¢¯åº¦
        dloss_dw2 = dloss_da2 * da2_dz2 * dz2_dw2
        dloss_db2 = dloss_da2 * da2_dz2 * dz2_db2
        dloss_dw1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_dw1
        dloss_db1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_db1

        # æ›´æ–°æƒé‡å’Œåç½®
        w2 -= learning_rate * dloss_dw2
        b2 -= learning_rate * dloss_db2
        w1 -= learning_rate * dloss_dw1
        b1 -= learning_rate * dloss_db1

        # ä¿å­˜æƒé‡å’Œåç½®å†å²
        w1_history.append(w1)
        b1_history.append(b1)
        w2_history.append(w2)
        b2_history.append(b2)

# ç»˜åˆ¶æŸå¤±å†å²
plt.figure(figsize=(14, 7))
plt.subplot(2, 3, 1)
plt.plot(loss_history, label='Loss over time')
plt.title('Loss History')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.legend()

# ç»˜åˆ¶æƒé‡å’Œåç½®å˜åŒ–
plt.subplot(2, 3, 2)
plt.plot(w1_history, label='w1')
plt.title('Weight w1 History')
plt.legend()

plt.subplot(2, 3, 3)
plt.plot(b1_history, label='b1')
plt.title('Bias b1 History')
plt.legend()

plt.subplot(2, 3, 4)
plt.plot(w2_history, label='w2')
plt.title('Weight w2 History')
plt.legend()

plt.subplot(2, 3, 5)
plt.plot(b2_history, label='b2')
plt.title('Bias b2 History')
plt.legend()

# ç»˜åˆ¶æ¨¡å‹æ‹Ÿåˆç»“æœ
plt.subplot(2, 3, 6)
plt.scatter(x, y, color='red', label='Data Points')  # åŸå§‹æ•°æ®ç‚¹
predicted = activation(activation(x * w1 + b1) * w2 + b2)
plt.plot(x, predicted, label='Fitted Line', color='blue')  # æ‹Ÿåˆæ›²çº¿
plt.title('Model Fit')
plt.legend()

plt.tight_layout()
plt.show()
```




# 3. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

### 1. æ¦‚è¿°

1. å®šä¹‰ï¼šå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMultilayer Perceptron, MLPï¼‰æ˜¯ä¸€ç§å‰é¦ˆç¥ç»ç½‘ç»œï¼Œé€šå¸¸ç”±è¾“å…¥å±‚ã€ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚å’Œè¾“å‡ºå±‚ç»„æˆã€‚æ¯ä¸€å±‚çš„ç¥ç»å…ƒä¸ä¸Šä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒç›¸è¿æ¥ã€‚

2. ç‰¹ç‚¹ï¼š

- å…¨è¿æ¥å±‚ï¼šMLP ä¸­æ¯ä¸ªç¥ç»å…ƒä¸å‰ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒç›¸è¿æ¥ï¼Œè¿™ç§°ä¸ºå…¨è¿æ¥å±‚ï¼ˆFully Connected Layerï¼‰ã€‚
- éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼šå¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ ReLUï¼ˆRectified Linear Unitï¼‰ã€Sigmoid å’Œ Tanhã€‚
- é€‚ç”¨äºç»“æ„åŒ–æ•°æ®ï¼šMLP é€šå¸¸ç”¨äºå¤„ç†ç»“æ„åŒ–æ•°æ®ï¼Œå¦‚è¡¨æ ¼æ•°æ®ã€‚
- ç®€å•çš„æ¶æ„ï¼šæ¶æ„è¾ƒä¸ºç®€å•ï¼Œé€‚ç”¨äºè¾ƒå°çš„æ•°æ®é›†å’Œä»»åŠ¡ã€‚



### 2. é¸¢å°¾èŠ± MLP

1. å†³ç­–è¾¹ç•Œ

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240627-153841.png" alt="Image Description" width="600">
</p>


2. æºä»£ç 

```py
# å¯¼å…¥å¿…è¦çš„åº“
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.decomposition import PCA

# åŠ è½½æ•°æ®é›†
iris = load_iris()
X = iris.data
y = iris.target

# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ä½¿ç”¨PCAé™ç»´åˆ°2ç»´
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# æ„å»ºMLPåˆ†ç±»å™¨
mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42)
mlp.fit(X_train, y_train)

# å†³ç­–è¾¹ç•Œçš„å¯è§†åŒ–
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))

# ä½¿ç”¨åˆ†ç±»å™¨å¯¹ç½‘æ ¼ç‚¹è¿›è¡Œé¢„æµ‹
Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
plt.figure(figsize=(10, 5))
plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='k', s=20)
plt.title('Decision Boundary after PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
```

ğŸ“Œ ä»£ç è§£é‡Š

- æ•°æ®æ ‡å‡†åŒ–ï¼šæˆ‘ä»¬ä½¿ç”¨StandardScalerå¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚
- PCAé™ç»´ï¼šä½¿ç”¨PCAå°†æ•°æ®ä»4ç»´é™åˆ°2ç»´ã€‚
- æ•°æ®é›†æ‹†åˆ†ï¼šå°†é™ç»´åçš„æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
- è®­ç»ƒMLPåˆ†ç±»å™¨ï¼šæ„å»ºå¹¶è®­ç»ƒä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªéšè—å±‚çš„MLPåˆ†ç±»å™¨ã€‚
- ç”Ÿæˆç½‘æ ¼å¹¶é¢„æµ‹ç±»åˆ«ï¼šåœ¨äºŒç»´ç‰¹å¾ç©ºé—´ä¸­ç”Ÿæˆä¸€ä¸ªç½‘æ ¼ç‚¹é›†åˆï¼Œå¹¶ä½¿ç”¨è®­ç»ƒå¥½çš„åˆ†ç±»å™¨å¯¹è¿™äº›ç½‘æ ¼ç‚¹è¿›è¡Œé¢„æµ‹ã€‚
- ç»˜åˆ¶å†³ç­–è¾¹ç•Œï¼šä½¿ç”¨contourfå‡½æ•°ç»˜åˆ¶å†³ç­–è¾¹ç•Œï¼Œå¹¶ä½¿ç”¨scatterå‡½æ•°ç»˜åˆ¶å®é™…æ•°æ®ç‚¹ã€‚


# 4. å·ç§¯ç¥ç»ç½‘ç»œ

### 1. å·ç§¯å±‚

1. å·ç§¯å±‚

2. å¡«å……

3. æ­¥å¹…


### 2. æ± åŒ–å±‚


### 3. LeNet

### 4. AlexNet





# 5. è‡ªç¼–ç å™¨





# 6. å¾ªç¯ç¥ç»ç½‘ç»œ











