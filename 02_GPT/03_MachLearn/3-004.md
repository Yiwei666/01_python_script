# 深度学习

# 1. 基本概念

### 1. 交叉熵和KL散度

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-142123.png" alt="Image Description" width="700">
</p>

### 2. 损失函数

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-163959.png" alt="Image Description" width="700">
</p>

### 3. 激活函数

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-163525.png" alt="Image Description" width="700">
</p>

- 图像

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-113622.png" alt="Image Description" width="700">
</p>

- 绘图代码

```py
import numpy as np
import matplotlib.pyplot as plt

# 定义Sigmoid函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 定义Tanh函数
def tanh(x):
    return np.tanh(x)

# 定义ReLU函数
def relu(x):
    return np.maximum(0, x)

# 定义Softmax函数
def softmax(x):
    e_x = np.exp(x - np.max(x))  # 减去最大值以提高数值稳定性
    return e_x / e_x.sum(axis=0)

# 生成x值
x = np.linspace(-10, 10, 100)

# 创建一个图形和四个子图（2x2布局）
fig, axs = plt.subplots(2, 2, figsize=(12, 10))

# 绘制Sigmoid函数
axs[0, 0].plot(x, sigmoid(x), label='Sigmoid', color='blue')
axs[0, 0].set_title('Sigmoid Activation Function')
axs[0, 0].grid(True)
axs[0, 0].set_xlabel('X')
axs[0, 0].set_ylabel('Y')

# 绘制Tanh函数
axs[0, 1].plot(x, tanh(x), label='Tanh', color='red')
axs[0, 1].set_title('Tanh Activation Function')
axs[0, 1].grid(True)
axs[0, 1].set_xlabel('X')
axs[0, 1].set_ylabel('Y')

# 绘制ReLU函数
axs[1, 0].plot(x, relu(x), label='ReLU', color='green')
axs[1, 0].set_title('ReLU Activation Function')
axs[1, 0].grid(True)
axs[1, 0].set_xlabel('X')
axs[1, 0].set_ylabel('Y')

# 绘制Softmax函数
# 注意：Softmax通常用于多类分类，这里为了示范，我们只是展示单变量的Softmax
axs[1, 1].plot(x, softmax(x), label='Softmax', color='purple')
axs[1, 1].set_title('Softmax Activation Function')
axs[1, 1].grid(True)
axs[1, 1].set_xlabel('X')
axs[1, 1].set_ylabel('Y')

# 为每个子图添加图例
for ax in np.ravel(axs):  # 使用 np.ravel 处理索引
    ax.legend()

# 自动调整子图间距
plt.tight_layout()

# 显示图形
plt.show()
```

### 4. 梯度下降和学习率

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-143101.png" alt="Image Description" width="700">
</p>

### 5. 随机梯度下降和小批量梯度下降

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-143727.png" alt="Image Description" width="700">
</p>


### 6. 误差反向传播法

1. 基本概念

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172253.png" alt="Image Description" width="700">
</p>

2. 理论推导

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172636.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172851.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-173024.png" alt="Image Description" width="700">
</p>

3. 实例化

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-174356.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-174749.png" alt="Image Description" width="700">
</p>





# 2. 神经网络

### 1. 反向传播演示

为了演示梯度下降在神经网络中的应用，我们可以通过一个简单的神经网络示例，其中包含一个输入层、一个隐藏层（只有一个神经元）和一个输出层（同样只有一个神经元），来预测一些线性数据。

假设我们有一个简单的线性关系 `y=2x+1`，我们的目标是训练一个神经网络来学习这个关系。

```py
import numpy as np
import matplotlib.pyplot as plt

# 使用NumPy生成线性数据，并添加一些高斯噪声
np.random.seed(42)  # 设置随机种子以确保结果可重复
x = np.linspace(-1, 1, 100)  # 生成100个等间隔的x值，范围从-1到1
y = 2 * x + 1 + np.random.randn(*x.shape) * 0.4  # 真实的y值基于线性关系2x + 1，并添加噪声

# 初始化神经网络参数：权重和偏置
w1 = np.random.randn()  # 隐藏层权重
b1 = np.random.randn()  # 隐藏层偏置
w2 = np.random.randn()  # 输出层权重
b2 = np.random.randn()  # 输出层偏置

# 设置学习率
learning_rate = 0.01

# 定义激活函数及其导数：这里使用线性激活函数
def activation(z):
    return z  # 线性激活

def activation_prime(z):
    return 1  # 线性激活函数的导数

# 训练过程
num_epochs = 100  # 设置迭代次数
for epoch in range(num_epochs):
    for i in range(len(x)):
        # 前向传播
        z1 = x[i] * w1 + b1  # 计算隐藏层的输入
        a1 = activation(z1)  # 计算隐藏层的输出
        z2 = a1 * w2 + b2  # 计算输出层的输入
        a2 = activation(z2)  # 计算输出层的输出

        # 计算损失（均方误差）
        loss = (a2 - y[i]) ** 2

        # 反向传播：计算损失函数关于各个参数的导数
        dloss_da2 = 2 * (a2 - y[i])
        da2_dz2 = activation_prime(z2)
        dz2_dw2 = a1
        dz2_db2 = 1
        dz2_da1 = w2
        da1_dz1 = activation_prime(z1)
        dz1_dw1 = x[i]
        dz1_db1 = 1

        # 计算梯度
        dloss_dw2 = dloss_da2 * da2_dz2 * dz2_dw2
        dloss_db2 = dloss_da2 * da2_dz2 * dz2_db2
        dloss_dw1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_dw1
        dloss_db1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_db1

        # 更新权重和偏置
        w2 -= learning_rate * dloss_dw2
        b2 -= learning_rate * dloss_db2
        w1 -= learning_rate * dloss_dw1
        b1 -= learning_rate * dloss_db1

# 绘制训练结果
predicted = activation(activation(x * w1 + b1) * w2 + b2)
plt.scatter(x, y, color='red', label='Data Points')  # 原始数据点
plt.plot(x, predicted, label='Fitted Line')  # 拟合曲线
plt.legend()
plt.show()
```

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-151840.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-151924.png" alt="Image Description" width="700">
</p>


### 2. 训练过程动态视图

💎 图像

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-153603.png" alt="Image Description" width="800">
</p>

1. 损失历史（Loss History）
   - 观察到的现象：损失值在开始时迅速下降，这表明模型在最初迭代中学习了大量的信息。随后，损失值趋于稳定，呈现出一种周期性的小波动。
   - 可能的解释：初始快速下降可能由于权重和偏置从随机值向更优解调整。稳定后的小波动可能是因为学习率相对较小或模型已接近最优解但还在微调。

2. 权重历史（Weight w1 and w2 History）
   - 权重 w1 和 w2：两个权重都显示出快速的初始变化，然后逐渐平稳。w1 和 w2 的变化趋势表明，模型参数在经过初期调整后基本稳定。
   - 权重调整：快速的初始变化可能是模型在试图找到适应数据的最佳方式。之后变化平缓可能意味着模型已经找到了合适的参数设置。

3. 偏置历史（Bias b1 and b2 History）
   - 偏置 b1 和 b2：偏置的变化趋势与权重类似，初始有较大的调整，随后变化幅度减小。b1 在调整后趋于稳定，而 b2 在初始跳跃后也趋于平稳。
   - 影响解释：偏置的调整帮助模型在特征空间中正确定位决策边界。稳定后的偏置表明模型在数据上的偏差已经较小。

4. 模型拟合（Model Fit）
   - 拟合效果：从模型拟合图看，拟合线（蓝色）与数据点（红色）非常接近，说明模型有效地学习了数据中的线性关系。
   - 效果评估：模型能够很好地预测或近似真实数据分布，这表明所选的网络架构和训练策略是有效的。



💎 绘图代码

```py
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

# 使用NumPy生成线性数据，并添加一些高斯噪声
np.random.seed(42)  # 设置随机种子以确保结果可重复
x = np.linspace(-1, 1, 100)  # 生成100个等间隔的x值，范围从-1到1
y = 2 * x + 1 + np.random.randn(*x.shape) * 0.4  # 真实的y值基于线性关系2x + 1，并添加噪声

# 初始化神经网络参数：权重和偏置
w1 = np.random.randn()  # 隐藏层权重
b1 = np.random.randn()  # 隐藏层偏置
w2 = np.random.randn()  # 输出层权重
b2 = np.random.randn()  # 输出层偏置

# 设置学习率
learning_rate = 0.01

# 定义激活函数及其导数：这里使用线性激活函数
def activation(z):
    return z  # 线性激活

def activation_prime(z):
    return 1  # 线性激活函数的导数

# 初始化记录损失和参数变化的列表
loss_history = []
w1_history, b1_history, w2_history, b2_history = [], [], [], []

# 训练过程
num_epochs = 100  # 设置迭代次数
for epoch in tqdm(range(num_epochs), desc="Training Progress"):
    for i in range(len(x)):
        # 前向传播
        z1 = x[i] * w1 + b1  # 计算隐藏层的输入
        a1 = activation(z1)  # 计算隐藏层的输出
        z2 = a1 * w2 + b2  # 计算输出层的输入
        a2 = activation(z2)  # 计算输出层的输出

        # 计算损失（均方误差）
        loss = (a2 - y[i])**2
        loss_history.append(loss)  # 记录每次的损失值

        # 反向传播：计算损失函数关于各个参数的导数
        dloss_da2 = 2 * (a2 - y[i])
        da2_dz2 = activation_prime(z2)
        dz2_dw2 = a1
        dz2_db2 = 1
        dz2_da1 = w2
        da1_dz1 = activation_prime(z1)
        dz1_dw1 = x[i]
        dz1_db1 = 1

        # 计算梯度
        dloss_dw2 = dloss_da2 * da2_dz2 * dz2_dw2
        dloss_db2 = dloss_da2 * da2_dz2 * dz2_db2
        dloss_dw1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_dw1
        dloss_db1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_db1

        # 更新权重和偏置
        w2 -= learning_rate * dloss_dw2
        b2 -= learning_rate * dloss_db2
        w1 -= learning_rate * dloss_dw1
        b1 -= learning_rate * dloss_db1

        # 保存权重和偏置历史
        w1_history.append(w1)
        b1_history.append(b1)
        w2_history.append(w2)
        b2_history.append(b2)

# 绘制损失历史
plt.figure(figsize=(14, 7))
plt.subplot(2, 3, 1)
plt.plot(loss_history, label='Loss over time')
plt.title('Loss History')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.legend()

# 绘制权重和偏置变化
plt.subplot(2, 3, 2)
plt.plot(w1_history, label='w1')
plt.title('Weight w1 History')
plt.legend()

plt.subplot(2, 3, 3)
plt.plot(b1_history, label='b1')
plt.title('Bias b1 History')
plt.legend()

plt.subplot(2, 3, 4)
plt.plot(w2_history, label='w2')
plt.title('Weight w2 History')
plt.legend()

plt.subplot(2, 3, 5)
plt.plot(b2_history, label='b2')
plt.title('Bias b2 History')
plt.legend()

# 绘制模型拟合结果
plt.subplot(2, 3, 6)
plt.scatter(x, y, color='red', label='Data Points')  # 原始数据点
predicted = activation(activation(x * w1 + b1) * w2 + b2)
plt.plot(x, predicted, label='Fitted Line', color='blue')  # 拟合曲线
plt.title('Model Fit')
plt.legend()

plt.tight_layout()
plt.show()
```




# 3. 多层感知机（MLP）

### 1. 概述

1. 定义：多层感知机（Multilayer Perceptron, MLP）是一种前馈神经网络，通常由输入层、一个或多个隐藏层和输出层组成。每一层的神经元与上一层的所有神经元相连接。

2. 特点：

- 全连接层：MLP 中每个神经元与前一层的所有神经元相连接，这称为全连接层（Fully Connected Layer）。
- 非线性激活函数：常用的激活函数包括 ReLU（Rectified Linear Unit）、Sigmoid 和 Tanh。
- 适用于结构化数据：MLP 通常用于处理结构化数据，如表格数据。
- 简单的架构：架构较为简单，适用于较小的数据集和任务。



### 2. 鸢尾花 MLP

1. 决策边界

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240627-153841.png" alt="Image Description" width="600">
</p>


2. 源代码

```py
# 导入必要的库
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.decomposition import PCA

# 加载数据集
iris = load_iris()
X = iris.data
y = iris.target

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 使用PCA降维到2维
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 将数据集拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# 构建MLP分类器
mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42)
mlp.fit(X_train, y_train)

# 决策边界的可视化
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))

# 使用分类器对网格点进行预测
Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# 绘制决策边界
plt.figure(figsize=(10, 5))
plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='k', s=20)
plt.title('Decision Boundary after PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
```

📌 代码解释

- 数据标准化：我们使用StandardScaler对数据进行标准化处理。
- PCA降维：使用PCA将数据从4维降到2维。
- 数据集拆分：将降维后的数据集拆分为训练集和测试集。
- 训练MLP分类器：构建并训练一个具有两个隐藏层的MLP分类器。
- 生成网格并预测类别：在二维特征空间中生成一个网格点集合，并使用训练好的分类器对这些网格点进行预测。
- 绘制决策边界：使用contourf函数绘制决策边界，并使用scatter函数绘制实际数据点。



# 4. 卷积神经网络（CNN）

### 1. 全连接深度神经网络（DNNs）

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-160849.png" alt="Image Description" width="700">
</p>


### 2. 滤波器和特征图

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-161554.png" alt="Image Description" width="700">
</p>



### 2. 卷积层

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-162208.png" alt="Image Description" width="700">
</p>

💎 **卷积层的基本构成**

1. 卷积操作：

   - 卷积层通过执行卷积操作来处理输入数据（通常是图像）。这种操作包括将一组小型的、可学习的滤波器（或称为卷积核）应用于输入数据。
   - 每个滤波器都在输入图像的全局或部分区域上滑动，对应的区域进行元素乘积后求和，生成特征图的一个像素点。这个过程在整个输入图像上重复进行，以形成完整的特征图。

2. 滤波器和特征图：

   - 滤波器负责从输入图像中提取特定类型的特征，如边缘、角点、纹理等。每个滤波器都对应于输出的一个特征图，显示了输入图像在该滤波器表示的特征上的响应强度和位置。
   - 在实际应用中，卷积层可以包含多个滤波器，每个滤波器产生一个独立的特征图，这些特征图堆叠在一起形成输出数据。

💎 **卷积层的关键参数**

3. 步长（Stride）：

   - 步长定义了滤波器在输入图像上滑动时每次移动的像素数。较大的步长会减小输出特征图的尺寸，而较小的步长能生成更详细的特征图。

4. 填充（Padding）：

   - 填充是指在输入图像的边界周围填充一定数量的像素（通常是0），以允许滤波器覆盖到图像的边缘部分。这有助于控制输出特征图的空间尺寸，并保持输入和输出尺寸的一致性。
     
💎 **卷积层的优点和功能**

5. 参数共享：

   - 在卷积层中，每个滤波器的参数（即卷积核的权重）在整个输入图像上是共享的。这种参数共享显著减少了模型的参数数量，降低了过拟合的风险，并提高了模型的泛化能力。

6. 局部连接：

   - 卷积层采用局部连接的策略，即每个神经元仅与输入数据的一个局部区域相连接。这利用了图像数据的局部空间关联性，使模型能够捕捉到局部特征信息，并减少了计算复杂度。




### 3. 池化层


### 4. LeNet

### 5. AlexNet







# 5. 自编码器





# 6. 循环神经网络（RNN）

### 1. 概述

1. 序列数据的定义
   - 序列数据是指多个样本之间存在顺序关系的数据类型。这与独立样本的处理不同，在序列数据中，每个样本的顺序对整个序列的理解和处理至关重要。

2. 序列数据的示例
   - 时间序列数据：如股票价格、天气数据等。
   - 语言数据：如句子中的单词序列。
   - 音频数据：如语音信号中的采样点序列。

3. 处理序列数据的挑战。处理序列数据的主要挑战在于如何捕捉和利用数据中存在的顺序关系。具体来说，挑战包括：
   - 捕捉长时间依赖关系：在长序列中，早期样本的信息可能对后期样本的处理很重要。
   - 上下文信息的保持：如何在处理每个样本时保留前面样本的信息，以便理解当前样本的含义。
   - 数据的顺序性：确保数据的顺序不会在处理过程中丢失或被打乱。

💎 **处理序列数据的方法**

1. 全连接神经网络

在最简单的情况下，全连接神经网络（FCN）可以用于处理序列数据，但这种方法存在显著的局限性：

- FCN 无法有效捕捉序列中的长时间依赖关系，因为每个输入样本都被独立处理，没有记忆机制来保留前面的信息。

2. 循环神经网络（RNNs）

RNNs 是为处理序列数据而设计的一种神经网络架构。其核心思想是引入状态（state）的概念，使网络能够记住先前的输入信息，并利用这些信息来影响当前的输出。

- 状态：RNNs 通过隐藏状态（hidden state）来保持对序列中前面样本的信息的记忆。每次输入一个样本时，RNN 更新其隐藏状态，并将其传递到下一时间步。
- 训练方法：通过反向传播算法（Backpropagation Through Time, BPTT）来训练RNNs。这种方法可以处理序列中的时间依赖关系，但也可能遇到梯度消失或爆炸的问题。

3. 序列数据处理的应用

RNNs 在处理序列数据时表现出色，其应用包括但不限于：

- 语言模型：预测句子中下一个单词或字符。
- 机器翻译：将一句话从一种语言翻译到另一种语言。
- 语音识别：将音频信号转化为文本。


### 2. 架构和原理

`循环神经网络（Recurrent Neural Networks, RNNs）`是一类设计用来识别序列数据模式的神经网络，如文本、基因组、手写文字、语音语言和传感器、股市以及政府机构提供的数值时间序列数据。与传统的前馈神经网络不同，RNNs具有形成有向循环的连接，使其能够保持之前输入的状态或记忆。这一特性使它们在输入顺序重要的任务中非常强大。

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-102637.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-103313.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-103507.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240703-103621.png" alt="Image Description" width="700">
</p>




### 2. 







