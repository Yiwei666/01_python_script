# 深度学习

# 1. 基本概念

### 1. 交叉熵和KL散度

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-142123.png" alt="Image Description" width="700">
</p>

### 2. 损失函数

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-163959.png" alt="Image Description" width="700">
</p>

### 3. 激活函数

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-163525.png" alt="Image Description" width="700">
</p>

- 图像

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-113622.png" alt="Image Description" width="700">
</p>

- 绘图代码

```py
import numpy as np
import matplotlib.pyplot as plt

# 定义Sigmoid函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 定义Tanh函数
def tanh(x):
    return np.tanh(x)

# 定义ReLU函数
def relu(x):
    return np.maximum(0, x)

# 定义Softmax函数
def softmax(x):
    e_x = np.exp(x - np.max(x))  # 减去最大值以提高数值稳定性
    return e_x / e_x.sum(axis=0)

# 生成x值
x = np.linspace(-10, 10, 100)

# 创建一个图形和四个子图（2x2布局）
fig, axs = plt.subplots(2, 2, figsize=(12, 10))

# 绘制Sigmoid函数
axs[0, 0].plot(x, sigmoid(x), label='Sigmoid', color='blue')
axs[0, 0].set_title('Sigmoid Activation Function')
axs[0, 0].grid(True)
axs[0, 0].set_xlabel('X')
axs[0, 0].set_ylabel('Y')

# 绘制Tanh函数
axs[0, 1].plot(x, tanh(x), label='Tanh', color='red')
axs[0, 1].set_title('Tanh Activation Function')
axs[0, 1].grid(True)
axs[0, 1].set_xlabel('X')
axs[0, 1].set_ylabel('Y')

# 绘制ReLU函数
axs[1, 0].plot(x, relu(x), label='ReLU', color='green')
axs[1, 0].set_title('ReLU Activation Function')
axs[1, 0].grid(True)
axs[1, 0].set_xlabel('X')
axs[1, 0].set_ylabel('Y')

# 绘制Softmax函数
# 注意：Softmax通常用于多类分类，这里为了示范，我们只是展示单变量的Softmax
axs[1, 1].plot(x, softmax(x), label='Softmax', color='purple')
axs[1, 1].set_title('Softmax Activation Function')
axs[1, 1].grid(True)
axs[1, 1].set_xlabel('X')
axs[1, 1].set_ylabel('Y')

# 为每个子图添加图例
for ax in np.ravel(axs):  # 使用 np.ravel 处理索引
    ax.legend()

# 自动调整子图间距
plt.tight_layout()

# 显示图形
plt.show()
```

### 4. 梯度下降和学习率

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-143101.png" alt="Image Description" width="700">
</p>

### 5. 随机梯度下降和小批量梯度下降

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240701-143727.png" alt="Image Description" width="700">
</p>


### 6. 误差反向传播法

1. 基本概念

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172253.png" alt="Image Description" width="700">
</p>

2. 理论推导

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172636.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-172851.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-173024.png" alt="Image Description" width="700">
</p>

3. 实例化

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-174356.png" alt="Image Description" width="700">
</p>

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240629-174749.png" alt="Image Description" width="700">
</p>





# 2. 多层感知机（MLP）

### 1. 概述

1. 定义：多层感知机（Multilayer Perceptron, MLP）是一种前馈神经网络，通常由输入层、一个或多个隐藏层和输出层组成。每一层的神经元与上一层的所有神经元相连接。

2. 特点：

- 全连接层：MLP 中每个神经元与前一层的所有神经元相连接，这称为全连接层（Fully Connected Layer）。
- 非线性激活函数：常用的激活函数包括 ReLU（Rectified Linear Unit）、Sigmoid 和 Tanh。
- 适用于结构化数据：MLP 通常用于处理结构化数据，如表格数据。
- 简单的架构：架构较为简单，适用于较小的数据集和任务。



### 2. 鸢尾花 MLP

1. 决策边界

<p align="center">
<img src="https://19640810.xyz/05_image/01_imageHost/20240627-153841.png" alt="Image Description" width="600">
</p>


2. 源代码

```py
# 导入必要的库
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.decomposition import PCA

# 加载数据集
iris = load_iris()
X = iris.data
y = iris.target

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 使用PCA降维到2维
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 将数据集拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# 构建MLP分类器
mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42)
mlp.fit(X_train, y_train)

# 决策边界的可视化
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))

# 使用分类器对网格点进行预测
Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# 绘制决策边界
plt.figure(figsize=(10, 5))
plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='k', s=20)
plt.title('Decision Boundary after PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
```

📌 代码解释

- 数据标准化：我们使用StandardScaler对数据进行标准化处理。
- PCA降维：使用PCA将数据从4维降到2维。
- 数据集拆分：将降维后的数据集拆分为训练集和测试集。
- 训练MLP分类器：构建并训练一个具有两个隐藏层的MLP分类器。
- 生成网格并预测类别：在二维特征空间中生成一个网格点集合，并使用训练好的分类器对这些网格点进行预测。
- 绘制决策边界：使用contourf函数绘制决策边界，并使用scatter函数绘制实际数据点。


# 3. 卷积神经网络

### 1. 卷积层

1. 卷积层

2. 填充

3. 步幅


### 2. 池化层


### 3. LeNet

### 4. AlexNet





# 4. 自编码器





# 5. 循环神经网络











